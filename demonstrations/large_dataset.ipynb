{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ae27f1",
   "metadata": {},
   "source": [
    "# Large Dataset Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54f5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from stelaro.data import synthetic\n",
    "\n",
    "DATA_DIRECTORY = \"../data/\"\n",
    "SUMMARY_DIRECTORY = DATA_DIRECTORY + \"ncbi_genome_summaries/\"\n",
    "NCBI_TAXONOMY_DIRECTORY = DATA_DIRECTORY + \"ncbi_taxonomy/\"\n",
    "DATASET_V1_DIRECTORY = DATA_DIRECTORY + \"version_1/\"\n",
    "DATASET_V1_COMPRESSED_DIRECTORY = DATASET_V1_DIRECTORY + \"compressed/\"\n",
    "\n",
    "\n",
    "def mkdir(path: str) -> None:\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "mkdir(DATA_DIRECTORY)\n",
    "mkdir(DATASET_V1_DIRECTORY)\n",
    "mkdir(DATASET_V1_COMPRESSED_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9728e",
   "metadata": {},
   "source": [
    "## 1. Compress The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426a559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [06:14<00:00, 13.86s/it]\n",
      "100%|██████████| 27/27 [05:45<00:00, 12.81s/it]\n",
      "100%|██████████| 27/27 [41:05<00:00, 91.32s/it] \n"
     ]
    }
   ],
   "source": [
    "LENGTH = 1500\n",
    "with open(\"../datasets/version_1_splits/map.json\", \"r\") as f:\n",
    "    index_to_taxonomic_label = json.load(f)\n",
    "\n",
    "for dataset_name in (\"validate\", \"test\", \"train\"):\n",
    "    with open(f\"../datasets/version_1_splits/{dataset_name}.json\", \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "    index_to_n_passes = {}\n",
    "    directory = DATASET_V1_COMPRESSED_DIRECTORY + dataset_name + \"/\"\n",
    "    mkdir(directory)\n",
    "    synthetic.compress_dataset(\n",
    "        dataset,\n",
    "        index_to_taxonomic_label,\n",
    "        \"../data/version_1/genomes/\",\n",
    "        LENGTH,\n",
    "        directory\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f6fd9",
   "metadata": {},
   "source": [
    "## 2. Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80aac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = synthetic.get_n_reads_in_compressed_dataset(\n",
    "    DATASET_V1_COMPRESSED_DIRECTORY + \"train/\",\n",
    ")\n",
    "ids = synthetic.get_random_identifiers(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa734fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5398 12157  2987  6817  6141   994 13232 13711  2553  8935  3209  3800\n",
      " 12401  5645  3268  1090  8832  6483  9312 12444 11964 13768 16315 14968\n",
      "  3098   407    71]\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "from numpy import bincount\n",
    "x, y = synthetic.sample_compressed_dataset(\n",
    "    DATASET_V1_COMPRESSED_DIRECTORY + \"train/\",\n",
    "    200_000,\n",
    "    1500,\n",
    "    ids,\n",
    "    400000\n",
    ")\n",
    "print(bincount(y))\n",
    "print(sum(bincount(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a206af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Viruses', 'Monodnaviria']\n",
      "5158834\n"
     ]
    }
   ],
   "source": [
    "with open(f\"../datasets/version_1_splits/train.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "\n",
    "taxon = dataset[-1]\n",
    "print(taxon[0])\n",
    "references = []\n",
    "for element in taxon[1]:\n",
    "    genus, ref = element\n",
    "    references += ref\n",
    "print(synthetic.evaluate_n_nucleotides(references))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc08ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 375)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.load(DATASET_V1_COMPRESSED_DIRECTORY + \"train/0_x.npy\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950054ab",
   "metadata": {},
   "source": [
    "## 3. Train Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33f08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stelaro import models\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from stelaro.models import feedforward, autoencoder, transformer\n",
    "from time import time\n",
    "\n",
    "LENGTH = 1500\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data = DataLoader(\n",
    "    synthetic.CompressedReadDataset(\n",
    "        \"../data/version_1/compressed/train/\", 10_000\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "validate_data = DataLoader(\n",
    "    synthetic.SyntheticReadDataset(\n",
    "        \"../data/version_1/compressed/validate/\", 10_000, LENGTH\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    synthetic.SyntheticReadDataset(\n",
    "        \"../data/version_1/compressed/test/\", 20_000, LENGTH\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "with open(\"../data/version_1/static/train/map.json\", \"r\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "\n",
    "def benchmark(classifier: models.BaseClassifier, name: str, n_max_reads=30_000):\n",
    "    train_data.dataset.reset()\n",
    "    parameters = classifier.get_parameters()\n",
    "    if parameters:\n",
    "        optimizer = Adam(classifier.get_parameters(), lr=0.001)\n",
    "        total_params = sum(param.numel() for param in parameters)\n",
    "        print(f\"Number of parameters: {total_params:_}\")\n",
    "    else:\n",
    "        optimizer = None\n",
    "    a = time()\n",
    "    losses, f1 = classifier.train_large_dataset(\n",
    "        train_data,\n",
    "        validate_data,\n",
    "        optimizer,\n",
    "        evaluation_interval=10_000,\n",
    "        n_max_reads=n_max_reads,\n",
    "        patience=3,\n",
    "    )\n",
    "    b = time()\n",
    "    print(f\"Training took {(b - a):.3f} s.\")\n",
    "    if losses:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        x = list(range(len(losses)))\n",
    "        ax[0].plot(x, losses, label=\"losses\")\n",
    "        ax[0].set(xlabel='Epochs', ylabel='Loss')\n",
    "        ax[0].set_title(\"Loss\")\n",
    "        ax[1].set(xlabel='Epochs', ylabel=\"f1\")\n",
    "        ax[1].set_title(\"F1 Score\")\n",
    "        r = 0\n",
    "        for f in f1:\n",
    "            ax[1].plot(x, f, label=f'Rank {r}')\n",
    "            r += 1\n",
    "        ax[1].legend()\n",
    "        fig.suptitle(f\"Classification Training for {name}\")\n",
    "        plt.show()\n",
    "    result = models.evaluate(classifier, test_data, \"cuda\", mapping, permute=False)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"Test results: {rounded_result}\")\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89076ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 234/68609 [00:03<19:15, 59.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 3.956 s.\n",
      "Test results: [0.31536, 0.025564]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\maitrise\\stelaro\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "d:\\maitrise\\stelaro\\venv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n",
      "d:\\maitrise\\stelaro\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "d:\\maitrise\\stelaro\\venv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  ys_types = set(type_of_target(x) for x in ys)\n"
     ]
    }
   ],
   "source": [
    "model = benchmark(\n",
    "    models.RandomClassifier(),\n",
    "    \"Random Classifier\",\n",
    "    n_max_reads=30_000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
