{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c39736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from stelaro.data import format, ncbi\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "DATA_DIRECTORY = \"../data/\"\n",
    "SUMMARY_DIRECTORY = DATA_DIRECTORY + \"ncbi_genome_summaries/\"\n",
    "NCBI_TAXONOMY_DIRECTORY = DATA_DIRECTORY + \"ncbi_taxonomy/\"\n",
    "BERTAX_DIRECTORY = DATA_DIRECTORY + \"bertax/final/\"\n",
    "BERTAX_DATASET_DIRECTORY = BERTAX_DIRECTORY + \"final_model_data_seperate_fasta_per_superkingdom/data/fass2/projects/fk_read_classification/dna_sequences/fragments/genomic_fragments_80_big/\"\n",
    "BERTAX_DOMAINS = (\n",
    "    \"Archaea_db.fa\",\n",
    "    \"Bacteria_db.fa\",\n",
    "    \"Eukaryota_db.fa\",\n",
    "    \"Viruses_db.fa\",\n",
    ")\n",
    "BERTAX_STATISTIC_DIRECTORY = BERTAX_DIRECTORY + \"statistics/\"\n",
    "SEQUENCE_LENGTH = 1500\n",
    "N_MINIMUM_READS_PER_TAXON = 10_000\n",
    "\n",
    "\n",
    "def mkdir(path: str) -> None:\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "mkdir(BERTAX_STATISTIC_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2394d",
   "metadata": {},
   "source": [
    "# One-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05230c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTAX_TRAIN = BERTAX_DIRECTORY + \"train/\"\n",
    "BERTAX_VALIDATION = BERTAX_DIRECTORY + \"validation/\"\n",
    "BERTAX_TEST = BERTAX_DIRECTORY + \"test/\"\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torch import nn\n",
    "\n",
    "from stelaro.data import format\n",
    "from stelaro import models\n",
    "\n",
    "LENGTH = 1500\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "train_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(BERTAX_TRAIN),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "validation_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(BERTAX_VALIDATION),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(BERTAX_TEST),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "with open(BERTAX_DIRECTORY + \"statistics/map.json\", \"r\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "\n",
    "def benchmark(\n",
    "        classifier: models.BaseClassifier,\n",
    "        name: str,\n",
    "        max_epochs: int = 20\n",
    "    ):\n",
    "    parameters = classifier.get_parameters()\n",
    "    if parameters:\n",
    "        optimizer = Adam(classifier.get_parameters(), lr=0.001)\n",
    "        total_params = sum(param.numel() for param in parameters)\n",
    "        print(f\"Number of parameters: {total_params:_}\")\n",
    "    else:\n",
    "        optimizer = None\n",
    "    a = time()\n",
    "    losses, f1, validation_losses = classifier.train(\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        optimizer,\n",
    "        max_n_epochs=max_epochs,\n",
    "        patience=3,\n",
    "        loss_function=nn.CrossEntropyLoss(),\n",
    "        loss_function_type=\"supervised\",\n",
    "        evaluation_interval=5000,\n",
    "    )\n",
    "    b = time()\n",
    "    print(f\"Training took {(b - a):.3f} s.\")\n",
    "    if losses:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        x = list(range(len(losses)))\n",
    "        ax[0].plot(x, losses, label=\"Training\")\n",
    "        ax[0].plot(x, validation_losses, label=\"Validation\")\n",
    "        ax[0].set(xlabel='Epochs', ylabel='Loss')\n",
    "        ax[0].set_title(\"Normalized Loss Against Epochs\")\n",
    "        ax[0].legend()\n",
    "        ax[1].set(xlabel='Epochs', ylabel=\"f1\")\n",
    "        ax[1].set_title(\"F1 Score\")\n",
    "        r = 0\n",
    "        for f in f1:\n",
    "            ax[1].plot(x, f, label=f'Rank {r}')\n",
    "            r += 1\n",
    "        ax[1].legend()\n",
    "        fig.suptitle(f\"Classification Training for {name}\")\n",
    "        plt.show()\n",
    "    result = models.evaluate(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"F1: {rounded_result}\")\n",
    "    result = models.evaluate_precision(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"Precision: {rounded_result}\")\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 391_857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [10:22<6:48:27,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.85659, 0.71649]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [19:26<00:00,  7.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 T loss: 3.36632. V loss: 2.25728. F1: [0.88917, 0.77206]. P: [0.88976, 0.78047] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5002/9196 [11:19<4:42:11,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.91593, 0.81432]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [20:34<00:00,  7.45it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 T loss: 1.89184. V loss: 1.68167. F1: [0.92152, 0.82997]. P: [0.91607, 0.83817] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5002/9196 [11:33<4:48:19,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93227, 0.84978]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [20:44<00:00,  7.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 T loss: 1.48514. V loss: 1.51054. F1: [0.93244, 0.846]. P: [0.92662, 0.85752] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5003/9196 [11:18<3:42:51,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93364, 0.8667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [20:23<00:00,  7.52it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 T loss: 1.29058. V loss: 1.32135. F1: [0.93738, 0.86708]. P: [0.93545, 0.87031] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [11:00<6:45:32,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.94699, 0.8788]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [20:09<00:00,  7.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/10 T loss: 1.17214. V loss: 1.18121. F1: [0.94918, 0.88245]. P: [0.95223, 0.88366] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5002/9196 [11:21<4:47:18,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.94922, 0.88268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [20:09<00:00,  7.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/10 T loss: 1.08505. V loss: 1.23265. F1: [0.94069, 0.87787]. P: [0.93593, 0.88513] Patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4548/9196 [09:18<09:31,  8.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n\u001b[32m     59\u001b[39m classifier = models.Classifier(LENGTH // \u001b[32m4\u001b[39m, mapping, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, MambaSequenceClassifier, \u001b[38;5;28mformat\u001b[39m.to_tetramers)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m model = \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMAMBA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m matrix = models.confusion_matrix(model, test_data, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, mapping)\n\u001b[32m     66\u001b[39m plt.matshow(matrix)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(classifier, name, max_epochs)\u001b[39m\n\u001b[32m     51\u001b[39m     optimizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     52\u001b[39m a = time()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m losses, f1, validation_losses = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msupervised\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m b = time()\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(b\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39ma)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/stelaro/models/__init__.py:618\u001b[39m, in \u001b[36mClassifier.train\u001b[39m\u001b[34m(self, train_loader, validate_loader, optimizer, max_n_epochs, patience, loss_function, loss_function_type, evaluation_interval)\u001b[39m\n\u001b[32m    616\u001b[39m loss.backward()\n\u001b[32m    617\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m losses[-\u001b[32m1\u001b[39m] += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress \u001b[38;5;129;01mand\u001b[39;00m progress % evaluation_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    620\u001b[39m     ps = estimate_precision(\u001b[38;5;28mself\u001b[39m, validate_loader, \u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mself\u001b[39m.mapping)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel, MambaConfig\n",
    "from mamba_ssm import Mamba\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 256, 128, 2, 16, 8, 2, 0.1, batch 128\n",
    "# F1: [0.94682, 0.88288]\n",
    "# Precision: [0.94012, 0.88521]\n",
    "\n",
    "# 256, 64, 2, 16, 8, 2, 0.1, batch 128\n",
    "# F1: [0.92456, 0.8333]\n",
    "# Precision: [0.93198, 0.84133]\n",
    "\n",
    "# 256, 128, 2, 16, 8, 2, 0.1, batch 64\n",
    "# F1: [0.94531, 0.88431]\n",
    "# Precision: [0.94236, 0.88785]\n",
    "\n",
    "# 256, 128, 2, 16, 8, 3, 0.1, batch 128\n",
    "# F1: [0.95005, 0.88783]\n",
    "# Precision: [0.94598, 0.89011]\n",
    "\n",
    "# 256, 256, 2, 16, 8, 2, 0.1, batch 128\n",
    "\n",
    "class MambaSequenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        num_classes: int,\n",
    "        vocab_size: int = 256,\n",
    "        d_model: int = 128,\n",
    "        n_layers: int = 2,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 8,\n",
    "        expand: int = 3,\n",
    "        pooling = \"mean\",\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.Tensor:\n",
    "        h = self.embedding(x).to(dtype=torch.get_default_dtype())\n",
    "        for block in self.layers:\n",
    "            h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "        h = self.norm(h)\n",
    "        pooled = h.mean(dim=1)  # [B, d_model]\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "classifier = models.Classifier(LENGTH // 4, mapping, \"cuda\", MambaSequenceClassifier, format.to_tetramers)\n",
    "model = benchmark(\n",
    "    classifier,\n",
    "    \"MAMBA\",\n",
    "    max_epochs=10,\n",
    ")\n",
    "matrix = models.confusion_matrix(model, test_data, \"cuda\", mapping)\n",
    "plt.matshow(matrix)\n",
    "plt.show()\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=500, threshold=np.inf)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d470f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: [0.95005, 0.88783]\n",
      "Precision: [0.94598, 0.89011]\n"
     ]
    }
   ],
   "source": [
    "result = models.evaluate(classifier, test_data, \"cuda\", mapping)\n",
    "rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "print(f\"F1: {rounded_result}\")\n",
    "result = models.evaluate_precision(classifier, test_data, \"cuda\", mapping)\n",
    "rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "print(f\"Precision: {rounded_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55a64d",
   "metadata": {},
   "source": [
    "# Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39735249",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTAX_TRAIN = BERTAX_DIRECTORY + \"train/\"\n",
    "BERTAX_VALIDATION = BERTAX_DIRECTORY + \"validation/\"\n",
    "BERTAX_TEST = BERTAX_DIRECTORY + \"test/\"\n",
    "\n",
    "from time import time\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "from stelaro import models\n",
    "from stelaro.models import autoencoder, feedforward, transformer\n",
    "\n",
    "LENGTH = 1500\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "with open(BERTAX_DIRECTORY + \"statistics/map.json\", \"r\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "\n",
    "train_data = DataLoader(\n",
    "    models.SyntheticMultiLevelTetramerDataset(\n",
    "        BERTAX_TRAIN,\n",
    "        mapping,\n",
    "        (),\n",
    "        1,\n",
    "        balance = True,\n",
    "        other_factor = 0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "validation_data = DataLoader(\n",
    "    models.SyntheticMultiLevelTetramerDataset(\n",
    "        BERTAX_VALIDATION,\n",
    "        mapping,\n",
    "        (),\n",
    "        1,\n",
    "        balance = False,\n",
    "        other_factor = 0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    models.SyntheticMultiLevelTetramerDataset(\n",
    "        BERTAX_TEST,\n",
    "        mapping,\n",
    "        (),\n",
    "        1,\n",
    "        balance = False,\n",
    "        other_factor = 0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "mapping = train_data.dataset.mapping\n",
    "\n",
    "\n",
    "def benchmark(\n",
    "        classifier: models.Classifier,\n",
    "        name: str,\n",
    "        max_epochs: int = 20,\n",
    "        learning_rate: float = 0.001\n",
    "    ):\n",
    "    parameters = classifier.get_parameters()\n",
    "    if parameters:\n",
    "        optimizer = Adam(classifier.get_parameters(), lr=learning_rate)\n",
    "        total_params = sum(param.numel() for param in parameters)\n",
    "        print(f\"Number of parameters: {total_params:_}\")\n",
    "    else:\n",
    "        optimizer = None\n",
    "    a = time()\n",
    "    losses, f1, validation_losses = classifier.train(\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        optimizer,\n",
    "        max_n_epochs=max_epochs,\n",
    "        patience=3,\n",
    "        loss_function=nn.CrossEntropyLoss(),\n",
    "        loss_function_type=\"supervised\",\n",
    "        evaluation_interval=5000,\n",
    "    )\n",
    "    b = time()\n",
    "    print(f\"Training took {(b - a):.3f} s.\")\n",
    "    if losses:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        x = list(range(len(losses)))\n",
    "        ax[0].plot(x, losses, label=\"Training\")\n",
    "        ax[0].plot(x, validation_losses, label=\"Validation\")\n",
    "        ax[0].set(xlabel='Epochs', ylabel='Loss')\n",
    "        ax[0].set_title(\"Normalized Loss Against Epochs\")\n",
    "        ax[0].legend()\n",
    "        ax[1].set(xlabel='Epochs', ylabel=\"f1\")\n",
    "        ax[1].set_title(\"F1 Score\")\n",
    "        r = 0\n",
    "        for f in f1:\n",
    "            ax[1].plot(x, f, label=f'Rank {r}')\n",
    "            r += 1\n",
    "        ax[1].legend()\n",
    "        fig.suptitle(f\"Classification Training for {name}\")\n",
    "        plt.show()\n",
    "    result = models.evaluate(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"Test F1 score: {rounded_result}\")\n",
    "\n",
    "    result = models.evaluate_precision(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"Test precision score: {rounded_result}\")\n",
    "\n",
    "    return classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
