{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c39736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from stelaro.data import format, ncbi\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "DATA_DIRECTORY = \"../data/\"\n",
    "SUMMARY_DIRECTORY = DATA_DIRECTORY + \"ncbi_genome_summaries/\"\n",
    "NCBI_TAXONOMY_DIRECTORY = DATA_DIRECTORY + \"ncbi_taxonomy/\"\n",
    "BERTAX_DIRECTORY = DATA_DIRECTORY + \"bertax/final/\"\n",
    "BERTAX_DATASET_DIRECTORY = BERTAX_DIRECTORY + \"final_model_data_seperate_fasta_per_superkingdom/data/fass2/projects/fk_read_classification/dna_sequences/fragments/genomic_fragments_80_big/\"\n",
    "BERTAX_DOMAINS = (\n",
    "    \"Archaea_db.fa\",\n",
    "    \"Bacteria_db.fa\",\n",
    "    \"Eukaryota_db.fa\",\n",
    "    \"Viruses_db.fa\",\n",
    ")\n",
    "BERTAX_STATISTIC_DIRECTORY = BERTAX_DIRECTORY + \"statistics/\"\n",
    "PROCESSED_PRETRAINING_DATA = DATA_DIRECTORY + \"bertax/pretraining/processed/\"\n",
    "SEQUENCE_LENGTH = 1500\n",
    "N_MINIMUM_READS_PER_TAXON = 10_000\n",
    "\n",
    "\n",
    "def mkdir(path: str) -> None:\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "mkdir(BERTAX_STATISTIC_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2394d",
   "metadata": {},
   "source": [
    "# One-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05230c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTAX_TRAIN = BERTAX_DIRECTORY + \"train/\"\n",
    "BERTAX_VALIDATION = BERTAX_DIRECTORY + \"validation/\"\n",
    "BERTAX_TEST = BERTAX_DIRECTORY + \"test/\"\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torch import nn, exp\n",
    "\n",
    "from stelaro.data import format\n",
    "from stelaro import models\n",
    "\n",
    "LENGTH = 1500\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "train_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(BERTAX_TRAIN),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "validation_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(BERTAX_VALIDATION),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(BERTAX_TEST),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "with open(BERTAX_DIRECTORY + \"statistics/map.json\", \"r\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "\n",
    "def benchmark(\n",
    "        classifier: models.BaseClassifier,\n",
    "        name: str,\n",
    "        max_epochs: int = 20,\n",
    "        loss_fn = nn.CrossEntropyLoss(),\n",
    "        evaluation_interval=5000,\n",
    "        learning_rate: float = 0.001\n",
    "    ):\n",
    "    parameters = classifier.get_parameters()\n",
    "    if parameters:\n",
    "        optimizer = Adam(classifier.get_parameters(), lr=0.001)\n",
    "        total_params = sum(param.numel() for param in parameters)\n",
    "        print(f\"Number of parameters: {total_params:_}\")\n",
    "    else:\n",
    "        optimizer = None\n",
    "    a = time()\n",
    "    losses, f1, validation_losses = classifier.train(\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        optimizer,\n",
    "        max_n_epochs=max_epochs,\n",
    "        patience=3,\n",
    "        loss_function=loss_fn,\n",
    "        loss_function_type=\"supervised\",\n",
    "        evaluation_interval=evaluation_interval\n",
    "    )\n",
    "    b = time()\n",
    "    print(f\"Training took {(b - a):.3f} s.\")\n",
    "    if losses:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        x = list(range(len(losses)))\n",
    "        ax[0].plot(x, losses, label=\"Training\")\n",
    "        ax[0].plot(x, validation_losses, label=\"Validation\")\n",
    "        ax[0].set(xlabel='Epochs', ylabel='Loss')\n",
    "        ax[0].set_title(\"Normalized Loss Against Epochs\")\n",
    "        ax[0].legend()\n",
    "        ax[1].set(xlabel='Epochs', ylabel=\"f1\")\n",
    "        ax[1].set_title(\"F1 Score\")\n",
    "        r = 0\n",
    "        for f in f1:\n",
    "            ax[1].plot(x, f, label=f'Rank {r}')\n",
    "            r += 1\n",
    "        ax[1].legend()\n",
    "        fig.suptitle(f\"Classification Training for {name}\")\n",
    "        plt.show()\n",
    "    result = models.evaluate(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"F1: {rounded_result}\")\n",
    "    result = models.evaluate_precision(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"Precision: {rounded_result}\")\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714d9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 388_785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2003/9196 [04:03<6:20:14,  3.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.57526, 0.22857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [08:44<5:56:33,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.65106, 0.35171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6001/9196 [13:25<5:32:00,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.69989, 0.45002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8003/9196 [17:58<1:02:40,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.72618, 0.48692]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [20:25<00:00,  7.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 T loss: 6.61341. V loss: 4.84294. F1: [0.7473, 0.50389]. P: [0.75105, 0.53675] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2003/9196 [04:20<6:17:59,  3.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.78049, 0.55876]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4003/9196 [08:41<4:33:56,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.79384, 0.58838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6003/9196 [13:06<2:47:26,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.80103, 0.60817]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8003/9196 [17:29<1:02:31,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.80639, 0.63857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [19:57<00:00,  7.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 T loss: 4.13323. V loss: 3.68842. F1: [0.80875, 0.63045]. P: [0.79989, 0.64341] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2003/9196 [04:27<6:15:09,  3.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.82103, 0.64781]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4003/9196 [08:48<4:28:45,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.82164, 0.66638]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6003/9196 [13:08<2:39:19,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.83035, 0.67798]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8003/9196 [17:25<1:00:39,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.8352, 0.68557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [19:49<00:00,  7.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 T loss: 3.35514. V loss: 3.08872. F1: [0.84296, 0.69312]. P: [0.83396, 0.70252] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2002/9196 [26:50<4:12:30,  2.11s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.85194, 0.70102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4003/9196 [30:21<2:18:18,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.85008, 0.70382]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6003/9196 [34:18<2:43:12,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.84822, 0.71657]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8003/9196 [38:46<1:03:00,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.85833, 0.72639]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [41:19<00:00,  3.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 T loss: 2.91102. V loss: 2.77387. F1: [0.86056, 0.7215]. P: [0.85924, 0.72444] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2001/9196 [04:37<11:33:42,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.86929, 0.73768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [09:18<5:57:19,  4.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.86356, 0.74011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6001/9196 [13:56<5:07:52,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.87261, 0.74341]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8001/9196 [18:34<1:55:36,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.87052, 0.74563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [21:10<00:00,  7.24it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/10 T loss: 2.59942. V loss: 2.52903. F1: [0.87302, 0.7456]. P: [0.86696, 0.75424] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2002/9196 [04:41<8:11:37,  4.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.86764, 0.75526]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [09:22<5:54:18,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.88441, 0.76443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6002/9196 [14:03<3:38:29,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.87482, 0.76742]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8002/9196 [18:45<1:21:35,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.88295, 0.77152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [21:21<00:00,  7.18it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/10 T loss: 2.36366. V loss: 2.31394. F1: [0.88486, 0.76933]. P: [0.88552, 0.77066] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2002/9196 [04:41<8:12:56,  4.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.88251, 0.77883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [09:27<5:54:43,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.89717, 0.77904]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6001/9196 [14:08<5:11:34,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.89149, 0.78166]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8002/9196 [18:49<1:21:39,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.89935, 0.78397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [21:25<00:00,  7.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/10 T loss: 2.16952. V loss: 2.14979. F1: [0.89476, 0.78544]. P: [0.90153, 0.78891] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2002/9196 [04:41<8:12:25,  4.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.89517, 0.79322]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [09:22<5:58:38,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.89351, 0.79669]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6002/9196 [14:03<3:39:00,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.90727, 0.79766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8002/9196 [18:49<1:25:25,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.90441, 0.80098]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [21:27<00:00,  7.14it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/10 T loss: 2.00815. V loss: 2.05332. F1: [0.90019, 0.79307]. P: [0.89525, 0.79748] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1462/9196 [03:12<16:56,  7.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 133\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhierarchical\u001b[39m(\u001b[38;5;28minput\u001b[39m, targets):\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m models.penalized_cross_entropy(\u001b[38;5;28minput\u001b[39m, targets, penalty_matrix)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m model = \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMAMBA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m matrix = models.confusion_matrix(model, test_data, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, mapping)\n\u001b[32m    140\u001b[39m plt.matshow(matrix)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(classifier, name, max_epochs, loss_fn)\u001b[39m\n\u001b[32m     52\u001b[39m     optimizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     53\u001b[39m a = time()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m losses, f1, validation_losses = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msupervised\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m b = time()\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(b\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39ma)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/stelaro/models/__init__.py:624\u001b[39m, in \u001b[36mClassifier.train\u001b[39m\u001b[34m(self, train_loader, validate_loader, optimizer, max_n_epochs, patience, loss_function, loss_function_type, evaluation_interval)\u001b[39m\n\u001b[32m    622\u001b[39m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m    623\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m losses[-\u001b[32m1\u001b[39m] += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress \u001b[38;5;129;01mand\u001b[39;00m progress % evaluation_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    626\u001b[39m     ps = estimate_precision(\u001b[38;5;28mself\u001b[39m, validate_loader, \u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mself\u001b[39m.mapping)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel, MambaConfig\n",
    "from mamba_ssm import Mamba\n",
    "from torch import nn\n",
    "\n",
    "# 256, 128, 2, 16, 8, 2, 0.1, batch 128, noClip, CEL\n",
    "# F1: [0.94682, 0.88288]\n",
    "# Precision: [0.94012, 0.88521]\n",
    "\n",
    "# 256, 64, 2, 16, 8, 2, 0.1, batch 128, noClip, CEL\n",
    "# F1: [0.92456, 0.8333]\n",
    "# Precision: [0.93198, 0.84133]\n",
    "\n",
    "# 256, 128, 2, 16, 8, 2, 0.1, batch 64, noClip, CEL\n",
    "# F1: [0.94531, 0.88431]\n",
    "# Precision: [0.94236, 0.88785]\n",
    "\n",
    "# 256, 128, 2, 16, 8, 3, 0.1, batch 128, noClip, CEL*\n",
    "# F1: [0.95005, 0.88783]\n",
    "# Precision: [0.94598, 0.89011]\n",
    "\n",
    "# 256, 256, 2, 16, 8, 2, 0.1, batch 64, 5 epochs, noClip, CEL\n",
    "# F1: [0.95149, 0.88721]\n",
    "# Precision: [0.95105, 0.88929]\n",
    "\n",
    "# 256, 128, 2, 16, 16, 2, 0.1, batch 128, noClip, CEL\n",
    "# F1: [0.95237, 0.88463]\n",
    "# Precision: [0.95195, 0.88635]\n",
    "\n",
    "# 256, 128, 2, 16, 16, 2, 0.1, batch 128, clip, focal mean\n",
    "# F1: [0.94285, 0.87642]\n",
    "# Precision: [0.93603, 0.87868]\n",
    "\n",
    "# 256, 128, 2, 16, 16, 2, 0.1, batch 128, clip, focal sum\n",
    "# F1: [0.94163, 0.87185]\n",
    "# Precision: [0.93829, 0.87553]\n",
    "\n",
    "# 256, 128, 2, 16, 16, 2, 0.1, batch 128, clip, penalty sum\n",
    "# F1: [0.91501, 0.68972]\n",
    "# Precision: [0.90453, 0.7091]\n",
    "\n",
    "# 256, 128, 2, 16, 8, 2, 0.1, batch 128, noClip, focal sum\n",
    "# F1: [0.93204, 0.85329]\n",
    "# Precision: [0.93095, 0.85767]\n",
    "\n",
    "# 256, 128, 3, 16, 8, 2, 0.1, batch 128, noClip, CEL\n",
    "# F1: [0.90227, 0.80133]\n",
    "# Precision: [0.89234, 0.80588]\n",
    "\n",
    "\n",
    "class MambaSequenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        num_classes: int,\n",
    "        vocab_size: int = 256,\n",
    "        d_model: int = 128,\n",
    "        n_layers: int = 3,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        pooling = \"mean\",\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.Tensor:\n",
    "        h = self.embedding(x).to(dtype=torch.get_default_dtype())\n",
    "        # h = h.to(torch.float16)\n",
    "        for block in self.layers:\n",
    "            h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "        h = self.norm(h)\n",
    "        pooled = h.mean(dim=1)  # [B, d_model]\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "classifier = models.Classifier(\n",
    "    LENGTH // 4,\n",
    "    mapping,\n",
    "    \"cuda\",\n",
    "    MambaSequenceClassifier,\n",
    "    format.to_tetramers,\n",
    "    True\n",
    ")\n",
    "# classifier.model = classifier.model.to(torch.float16)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def focal_loss(inputs, targets, alpha=1, gamma=2):\n",
    "    num_classes = inputs.shape[1]\n",
    "    targets = F.one_hot(targets, num_classes=num_classes).float()\n",
    "    bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    pt = exp(-bce_loss)  # Convert BCE loss to probability\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * bce_loss  # Apply focal adjustment\n",
    "    return focal_loss.sum()\n",
    "\n",
    "\n",
    "def create_penalty_matrix(mapping):\n",
    "    d = len(mapping)\n",
    "    t = torch.zeros((d, d))\n",
    "    n_ranks = len(mapping['0'])\n",
    "    for i in mapping:\n",
    "        for j in range(d):\n",
    "            j = str(j)\n",
    "            union_length = 0\n",
    "            for a, b in zip(mapping[i], mapping[j]):\n",
    "                if a == b:\n",
    "                    union_length += 1\n",
    "                else:\n",
    "                    break\n",
    "            penalty = ((n_ranks - union_length) / n_ranks) ** 0.5\n",
    "            t[int(i), int(j)] = penalty\n",
    "    return t\n",
    "penalty_matrix = create_penalty_matrix(mapping).to(\"cuda\")\n",
    "def penalty_cross_entropy(y_pred, y_true):\n",
    "    probs = F.softmax(y_pred, dim=-1)  # [B, M]\n",
    "    penalty_rows = penalty_matrix[y_true]  # [B, M]\n",
    "    loss = -torch.sum((1 - penalty_rows) * torch.log(probs + 1e-12), dim=-1)\n",
    "    return loss.sum()\n",
    "def hierarchical(input, targets):\n",
    "    return models.penalized_cross_entropy(input, targets, penalty_matrix)\n",
    "\n",
    "\n",
    "model = benchmark(\n",
    "    classifier,\n",
    "    \"MAMBA\",\n",
    "    max_epochs=10,\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    ")\n",
    "matrix = models.confusion_matrix(model, test_data, \"cuda\", mapping)\n",
    "plt.matshow(matrix)\n",
    "plt.show()\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=500, threshold=np.inf)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d470f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: [0.90227, 0.80133]\n",
      "Precision: [0.89234, 0.80588]\n"
     ]
    }
   ],
   "source": [
    "result = models.evaluate(classifier, test_data, \"cuda\", mapping)\n",
    "rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "print(f\"F1: {rounded_result}\")\n",
    "result = models.evaluate_precision(classifier, test_data, \"cuda\", mapping)\n",
    "rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "print(f\"Precision: {rounded_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a809338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': ['Archaea', 'Methanobacteriota'], '1': ['Archaea', 'Nitrososphaerota'], '2': ['Archaea', 'Thermoplasmatota'], '3': ['Archaea', 'Thermoproteota'], '4': ['Bacteria', 'Actinomycetota'], '5': ['Bacteria', 'Aquificota'], '6': ['Bacteria', 'Bacillota'], '7': ['Bacteria', 'Bacteroidota'], '8': ['Bacteria', 'Bdellovibrionota'], '9': ['Bacteria', 'Campylobacterota'], '10': ['Bacteria', 'Chlamydiota'], '11': ['Bacteria', 'Chlorobiota'], '12': ['Bacteria', 'Chloroflexota'], '13': ['Bacteria', 'Cyanobacteriota'], '14': ['Bacteria', 'Deinococcota'], '15': ['Bacteria', 'Fusobacteriota'], '16': ['Bacteria', 'Gemmatimonadota'], '17': ['Bacteria', 'Lentisphaerota'], '18': ['Bacteria', 'Mycoplasmatota'], '19': ['Bacteria', 'Myxococcota'], '20': ['Bacteria', 'Nitrospirota'], '21': ['Bacteria', 'Planctomycetota'], '22': ['Bacteria', 'Pseudomonadota'], '23': ['Bacteria', 'Rhodothermota'], '24': ['Bacteria', 'Spirochaetota'], '25': ['Bacteria', 'Thermodesulfobacteriota'], '26': ['Bacteria', 'Thermotogota'], '27': ['Bacteria', 'Verrucomicrobiota'], '28': ['Eukaryota', 'Apicomplexa'], '29': ['Eukaryota', 'Arthropoda'], '30': ['Eukaryota', 'Ascomycota'], '31': ['Eukaryota', 'Bacillariophyta'], '32': ['Eukaryota', 'Basidiomycota'], '33': ['Eukaryota', 'Chlorophyta'], '34': ['Eukaryota', 'Chordata'], '35': ['Eukaryota', 'Euglenozoa'], '36': ['Eukaryota', 'Evosea'], '37': ['Eukaryota', 'Mollusca'], '38': ['Eukaryota', 'Nematoda'], '39': ['Eukaryota', 'Platyhelminthes'], '40': ['Eukaryota', 'Rhodophyta'], '41': ['Eukaryota', 'Streptophyta'], '42': ['Viruses', 'Artverviricota'], '43': ['Viruses', 'Kitrinoviricota'], '44': ['Viruses', 'Negarnaviricota'], '45': ['Viruses', 'Peploviricota'], '46': ['Viruses', 'Pisuviricota'], '47': ['Viruses', 'Taleaviricota'], '48': ['Viruses', 'Uroviricota']}\n",
      "tensor([[0.0000, 0.5000, 0.5000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.5000, 0.0000, 0.5000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.5000, 0.5000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.5000, 0.0000, 0.5000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.5000, 0.5000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(mapping)\n",
    "m = models.create_penalty_matrix(mapping)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af506c",
   "metadata": {},
   "source": [
    "# 1-NT Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32616ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 69_873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [28:50<8:21:51,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.87169, 0.7341]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [1:36:26<00:00,  1.59it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halting evaluation after 28032 data points.\n",
      "1/10 T loss: 13.99071. V loss: 10.32095. F1: [0.86647, 0.74595]. P: [0.88074, 0.77692] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5000/9196 [1:05:05<51:28,  1.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halting evaluation after 38144 data points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [1:05:36<11:29:05,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.90584, 0.8145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [1:57:10<00:00,  1.31it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halting evaluation after 38144 data points.\n",
      "2/10 T loss: 8.43410. V loss: 8.98794. F1: [0.88082, 0.78494]. P: [0.89169, 0.80668] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 94/9196 [01:11<1:54:40,  1.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n\u001b[32m     43\u001b[39m classifier = models.Classifier(LENGTH, mapping, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, MambaSequenceClassifier, \u001b[38;5;28mformat\u001b[39m.to_digits)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m model = \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMAMBA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m matrix = models.confusion_matrix(model, test_data, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, mapping)\n\u001b[32m     50\u001b[39m plt.matshow(matrix)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(classifier, name, max_epochs)\u001b[39m\n\u001b[32m     51\u001b[39m     optimizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     52\u001b[39m a = time()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m losses, f1, validation_losses = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msupervised\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m b = time()\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(b\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39ma)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/stelaro/models/__init__.py:618\u001b[39m, in \u001b[36mClassifier.train\u001b[39m\u001b[34m(self, train_loader, validate_loader, optimizer, max_n_epochs, patience, loss_function, loss_function_type, evaluation_interval)\u001b[39m\n\u001b[32m    616\u001b[39m loss.backward()\n\u001b[32m    617\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m losses[-\u001b[32m1\u001b[39m] += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress \u001b[38;5;129;01mand\u001b[39;00m progress % evaluation_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    620\u001b[39m     ps = estimate_precision(\u001b[38;5;28mself\u001b[39m, validate_loader, \u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mself\u001b[39m.mapping)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "from torch import nn\n",
    "\n",
    "# 256, 64, 2, 16, 8, 2, 0.1, batch 128\n",
    "# F1: [0.88082, 0.78494]\n",
    "# P: [0.89169, 0.80668]\n",
    "\n",
    "class MambaSequenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        num_classes: int,\n",
    "        vocab_size: int = 4,\n",
    "        d_model: int = 64,\n",
    "        n_layers: int = 2,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 8,\n",
    "        expand: int = 2,\n",
    "        pooling = \"mean\",\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.Tensor:\n",
    "        h = self.embedding(x).to(dtype=torch.get_default_dtype())\n",
    "        for block in self.layers:\n",
    "            h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "        h = self.norm(h)\n",
    "        pooled = h.mean(dim=1)  # [B, d_model]\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "classifier = models.Classifier(LENGTH, mapping, \"cuda\", MambaSequenceClassifier, format.to_digits)\n",
    "model = benchmark(\n",
    "    classifier,\n",
    "    \"MAMBA\",\n",
    "    max_epochs=10,\n",
    ")\n",
    "matrix = models.confusion_matrix(model, test_data, \"cuda\", mapping)\n",
    "plt.matshow(matrix)\n",
    "plt.show()\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=500, threshold=np.inf)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edcb9cf",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77852ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain pretraining data\n",
    "DATA_DIRECTORY = \"../data/\"\n",
    "BERTAX_DIRECTORY = DATA_DIRECTORY + \"bertax/pretraining/\"\n",
    "BERTAX_DATASET_DIRECTORY = BERTAX_DIRECTORY + \"pretraining_dataset/data/fass2/projects/fk_read_classification/dna_sequences/fragments/genomic_fragments_80/\"\n",
    "BERTAX_DOMAINS = (\n",
    "    \"Archaea_db.fa\",\n",
    "    \"Bacteria_db.fa\",\n",
    "    \"Eukaryota_db.fa\",\n",
    "    \"Viruses_db.fa\",\n",
    ")\n",
    "\n",
    "total = 0\n",
    "for domain in BERTAX_DOMAINS:\n",
    "    with open(BERTAX_DATASET_DIRECTORY + domain, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                total += 1\n",
    "    print(f\"Domain: {domain}. Total: {total}\")\n",
    "\n",
    "x = np.zeros((total, 1500 // 4), dtype=np.uint8)\n",
    "mkdir(PROCESSED_PRETRAINING_DATA)\n",
    "i = 0\n",
    "for domain in BERTAX_DOMAINS:\n",
    "    with open(BERTAX_DATASET_DIRECTORY + domain, \"r\") as f:\n",
    "        for line in f:\n",
    "            if not line.startswith(\">\"):\n",
    "                sequence = line.strip().upper()\n",
    "                characters = set(sequence)\n",
    "                if len(characters) == 4:\n",
    "                    encoding = format.encode_tetramer(sequence)\n",
    "                    x[i] = encoding\n",
    "                    i += 1\n",
    "np.save(PROCESSED_PRETRAINING_DATA + \"/x.npy\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "946663b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning:\n",
      "Number of parameters: 389_938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2001/9196 [03:41<9:12:55,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.80326, 0.64982]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4001/9196 [07:23<6:36:08,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.85748, 0.70958]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6001/9196 [11:08<4:09:57,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.88819, 0.7525]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8001/9196 [15:02<1:41:36,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.89519, 0.77108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [17:18<00:00,  8.86it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 T loss: 3.19294. V loss: 2.32218. F1: [0.89688, 0.77193]. P: [0.90517, 0.78354] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2002/9196 [04:06<7:58:51,  3.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.89802, 0.80257]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [08:26<6:06:06,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.90943, 0.80979]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6002/9196 [12:33<3:16:53,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.92629, 0.81882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8002/9196 [16:39<1:16:15,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.90996, 0.82479]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [19:04<00:00,  8.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 T loss: 1.95762. V loss: 1.75689. F1: [0.92233, 0.82708]. P: [0.92497, 0.83141] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2002/9196 [03:59<7:22:55,  3.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.91814, 0.83478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [08:01<5:21:18,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93004, 0.84943]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6001/9196 [12:09<4:51:13,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93896, 0.84498]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8002/9196 [16:12<1:16:59,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93467, 0.85354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [18:24<00:00,  8.32it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 T loss: 1.57684. V loss: 1.56207. F1: [0.93461, 0.84664]. P: [0.93917, 0.85025] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2002/9196 [04:02<8:10:50,  4.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93231, 0.85553]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4002/9196 [08:11<5:34:05,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93944, 0.85901]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4622/9196 [09:30<09:24,  8.10it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# classifier = models.Classifier(\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m#     LENGTH // 4,\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m#     mapping,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#     mlm_probability=0.25\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFine-tuning:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m model = \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMAMBA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\n\u001b[32m    139\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m matrix = models.confusion_matrix(model, test_data, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, mapping)\n\u001b[32m    141\u001b[39m plt.matshow(matrix)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(classifier, name, max_epochs, loss_fn, evaluation_interval, learning_rate)\u001b[39m\n\u001b[32m     54\u001b[39m     optimizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     55\u001b[39m a = time()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m losses, f1, validation_losses = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msupervised\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_interval\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m b = time()\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(b\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39ma)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/stelaro/models/__init__.py:778\u001b[39m, in \u001b[36mClassifier.train\u001b[39m\u001b[34m(self, train_loader, validate_loader, optimizer, max_n_epochs, patience, loss_function, loss_function_type, evaluation_interval)\u001b[39m\n\u001b[32m    776\u001b[39m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m    777\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m losses[-\u001b[32m1\u001b[39m] += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress \u001b[38;5;129;01mand\u001b[39;00m progress % evaluation_interval == \u001b[32m0\u001b[39m:\n\u001b[32m    780\u001b[39m     ps = estimate_precision(\u001b[38;5;28mself\u001b[39m, validate_loader, \u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mself\u001b[39m.mapping)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, AdamW\n",
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel, MambaConfig\n",
    "from mamba_ssm import Mamba\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 128, 2, 16, 4, 2, mean, 0.1, noClip, pretraining: 10_000 -> 0.15 MLM\n",
    "# F1: [0.9365, 0.86395]\n",
    "# Precision: [0.93262, 0.87061]\n",
    "\n",
    "# 128, 2, 16, 4, 2, mean, 0.1, noClip, pretraining: 20_000 -> 0.15 MLM\n",
    "# Precision: [0.93845, 0.86496] (incomplete training - no difference with 10k steps)\n",
    "# Lowest MLM loss: 0.1352\n",
    "# Fine-tuning:\n",
    "#   Epoch 1: F1: [0.88529, 0.75644]. P: [0.89593, 0.76523]\n",
    "#   Epoch 2: F1: [0.91113, 0.81615]. P: [0.91365, 0.82102]\n",
    "\n",
    "# 128, 2, 16, 4, 2, mean, 0.1, noClip, pretraining: 100_000 -> 0.15 MLM\n",
    "# 1/10 T loss: 2.97148. V loss: 2.46173. F1: [0.87928, 0.75458]. P: [0.87739, 0.77189] Patience: 3\n",
    "# 2/10 T loss: 1.97898. V loss: 1.84719. F1: [0.91845, 0.82066]. P: [0.92227, 0.82479] Patience: 3\n",
    "# 5/10 T loss: 1.37992. V loss: 1.43371. F1: [0.9326, 0.86184]. P: [0.92561, 0.86369]\n",
    "\n",
    "# 128, 2, 16, 4, 2, mean, 0.1, noClip, pretraining: 50_000 new -> 0.15 MLM\n",
    "# 1/10 T loss: 3.11947. V loss: 2.29345. F1: [0.89625, 0.77654]. P: [0.89769, 0.78591] Patience: 3\n",
    "# 2/10 T loss: 2.04814. V loss: 1.93529. F1: [0.91427, 0.81052]. P: [0.913, 0.81998] Patience: 3\n",
    "# 3/10 T loss: 1.70350. V loss: 1.71731. F1: [0.92385, 0.83688]. P: [0.9214, 0.84447] Patience: 3\n",
    "\n",
    "# 128, 2, 16, 4, 2, mean, 0.1, noClip, pretraining: 40_000 new -> 0.15 MLM, lr = 0.0001\n",
    "# 1/10 T loss: 3.12365. V loss: 2.35835. F1: [0.8859, 0.76903]. P: [0.88096, 0.77557] Patience: 3\n",
    "# 2/10 T loss: 1.99888. V loss: 1.85153. F1: [0.9151, 0.8191]. P: [0.9125, 0.82259] Patience: 3\n",
    "# 3/10 T loss: 1.68860. V loss: 1.62224. F1: [0.92761, 0.84333]. P: [0.92996, 0.84681] Patience: 3\n",
    "\n",
    "# 128, 3 (norms), 16, 4, 2, mean, 0.1, clip, pretraining: 10_000 new -> 0.2 MLM, lr = 0.0001\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm + residual connection\n",
    "        return x + self.mamba(self.norm(x))\n",
    "\n",
    "\n",
    "class MambaSequenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        num_classes: int,\n",
    "        vocab_size: int = 256,\n",
    "        d_model: int = 128,\n",
    "        n_layers: int = 3,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        pooling = \"mean\",\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            # Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            MambaBlock(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Conv1d(128, 256, kernel_size=5, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(N * 256, num_classes),\n",
    "        # )\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size + 1)\n",
    "        self.mlm_head.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, x: torch.LongTensor, mlm=None) -> torch.Tensor:\n",
    "        h = self.embedding(x)#.to(dtype=torch.get_default_dtype())\n",
    "        for block in self.layers:\n",
    "            h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "        h = self.norm(h)\n",
    "        if mlm is not None:\n",
    "            logits = self.mlm_head(h)  # [B, L, vocab_size + 1]\n",
    "            return logits\n",
    "        else:\n",
    "            # pooled = h[:, 0]\n",
    "            pooled = h.mean(dim=1)  # [B, d_model]\n",
    "            pooled = self.dropout(pooled)\n",
    "            # pooled = pooled.permute(0, 2, 1)\n",
    "            logits = self.classifier(pooled)  # [B, num_classes]\n",
    "            return logits\n",
    "\n",
    "\n",
    "classifier = models.Classifier(\n",
    "    LENGTH // 4,\n",
    "    mapping,\n",
    "    \"cuda\",\n",
    "    MambaSequenceClassifier,\n",
    "    format.to_tetramers,\n",
    "    True\n",
    ")\n",
    "\n",
    "print(\"Pretraining:\")\n",
    "optimizer = AdamW(classifier.get_parameters(), lr=0.001)\n",
    "pretraining_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(PROCESSED_PRETRAINING_DATA, labels=False),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "classifier.pretrain(\n",
    "    pretraining_data,\n",
    "    optimizer,\n",
    "    10_000,\n",
    "    256,\n",
    "    patience=3,\n",
    "    mlm_probability=0.25\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning:\")\n",
    "model = benchmark(\n",
    "    classifier,\n",
    "    \"MAMBA\",\n",
    "    max_epochs=10,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    evaluation_interval=2000,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "matrix = models.confusion_matrix(model, test_data, \"cuda\", mapping)\n",
    "plt.matshow(matrix)\n",
    "plt.show()\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=500, threshold=np.inf)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d66c52",
   "metadata": {},
   "source": [
    "# Frozen Initial Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bc93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19473 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1. Epoch: 1. MLM loss: 5.7056. Patience: 3\n",
      "Mean entropy: 5.3827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/19473 [00:04<10:46:28,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted masked tokens: 20 / 4757 (0.42043 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 250/19473 [00:53<1:04:14,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 250. Epoch: 1. MLM loss: 5.3718. Patience: 3\n",
      "Mean entropy: 5.3696\n",
      "Correctly predicted masked tokens: 76 / 4741 (1.603 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 500/19473 [01:43<1:04:09,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500. Epoch: 1. MLM loss: 2.6515. Patience: 3\n",
      "Mean entropy: 5.27\n",
      "Correctly predicted masked tokens: 124 / 4776 (2.5963 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1000/19473 [03:25<1:03:04,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000. Epoch: 1. MLM loss: 2.6291. Patience: 3\n",
      "Mean entropy: 5.024\n",
      "Correctly predicted masked tokens: 171 / 4780 (3.5774 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1500/19473 [05:07<1:01:34,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500. Epoch: 1. MLM loss: 1.7436. Patience: 3\n",
      "Mean entropy: 4.7029\n",
      "Correctly predicted masked tokens: 93 / 4859 (1.914 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2000/19473 [06:50<59:42,  4.88it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000. Epoch: 1. MLM loss: 1.3040. Patience: 3\n",
      "Mean entropy: 4.4719\n",
      "Correctly predicted masked tokens: 129 / 4840 (2.6653 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2500/19473 [08:32<57:57,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2500. Epoch: 1. MLM loss: 1.0409. Patience: 3\n",
      "Mean entropy: 4.2244\n",
      "Correctly predicted masked tokens: 121 / 4733 (2.5565 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3000/19473 [10:14<56:29,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3000. Epoch: 1. MLM loss: 0.8664. Patience: 3\n",
      "Mean entropy: 3.8972\n",
      "Correctly predicted masked tokens: 161 / 4830 (3.3333 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3500/19473 [11:57<54:48,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3500. Epoch: 1. MLM loss: 0.7419. Patience: 3\n",
      "Mean entropy: 3.9608\n",
      "Correctly predicted masked tokens: 130 / 4820 (2.6971 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4000/19473 [13:39<53:06,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000. Epoch: 1. MLM loss: 0.6484. Patience: 3\n",
      "Mean entropy: 3.7569\n",
      "Correctly predicted masked tokens: 156 / 4883 (3.1948 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 4500/19473 [15:22<51:43,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4500. Epoch: 1. MLM loss: 0.5759. Patience: 3\n",
      "Mean entropy: 3.605\n",
      "Correctly predicted masked tokens: 175 / 4820 (3.6307 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 5000/19473 [17:06<50:14,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000. Epoch: 1. MLM loss: 0.5182. Patience: 3\n",
      "Mean entropy: 3.726\n",
      "Correctly predicted masked tokens: 153 / 4887 (3.1308 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5500/19473 [18:54<48:09,  4.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5500. Epoch: 1. MLM loss: 0.4708. Patience: 3\n",
      "Mean entropy: 3.7266\n",
      "Correctly predicted masked tokens: 100 / 4751 (2.1048 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 6000/19473 [20:37<46:25,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6000. Epoch: 1. MLM loss: 0.4314. Patience: 3\n",
      "Mean entropy: 3.6123\n",
      "Correctly predicted masked tokens: 122 / 4807 (2.538 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6500/19473 [22:20<44:39,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6500. Epoch: 1. MLM loss: 0.3980. Patience: 3\n",
      "Mean entropy: 3.285\n",
      "Correctly predicted masked tokens: 124 / 4875 (2.5436 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 7000/19473 [24:03<42:54,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7000. Epoch: 1. MLM loss: 0.3693. Patience: 3\n",
      "Mean entropy: 3.6564\n",
      "Correctly predicted masked tokens: 140 / 4874 (2.8724 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 7500/19473 [25:45<41:05,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7500. Epoch: 1. MLM loss: 0.3446. Patience: 3\n",
      "Mean entropy: 3.7492\n",
      "Correctly predicted masked tokens: 114 / 4879 (2.3365 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 8000/19473 [27:28<39:32,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8000. Epoch: 1. MLM loss: 0.3231. Patience: 3\n",
      "Mean entropy: 3.6094\n",
      "Correctly predicted masked tokens: 111 / 4844 (2.2915 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 8500/19473 [29:11<37:39,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8500. Epoch: 1. MLM loss: 0.3037. Patience: 3\n",
      "Mean entropy: 3.7117\n",
      "Correctly predicted masked tokens: 117 / 4814 (2.4304 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 9000/19473 [30:54<36:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9000. Epoch: 1. MLM loss: 0.2869. Patience: 3\n",
      "Mean entropy: 3.7194\n",
      "Correctly predicted masked tokens: 121 / 4755 (2.5447 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 9500/19473 [32:36<34:16,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9500. Epoch: 1. MLM loss: 0.2717. Patience: 3\n",
      "Mean entropy: 3.745\n",
      "Correctly predicted masked tokens: 175 / 4795 (3.6496 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 10000/19473 [34:19<32:29,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000. Epoch: 1. MLM loss: 0.2581. Patience: 3\n",
      "Mean entropy: 3.8241\n",
      "Correctly predicted masked tokens: 115 / 4830 (2.381 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 10500/19473 [36:02<30:46,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10500. Epoch: 1. MLM loss: 0.2458. Patience: 3\n",
      "Mean entropy: 3.9131\n",
      "Correctly predicted masked tokens: 142 / 4738 (2.997 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 11000/19473 [37:44<29:05,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11000. Epoch: 1. MLM loss: 0.2345. Patience: 3\n",
      "Mean entropy: 4.029\n",
      "Correctly predicted masked tokens: 172 / 4826 (3.564 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 11500/19473 [39:27<27:24,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11500. Epoch: 1. MLM loss: 0.2243. Patience: 3\n",
      "Mean entropy: 4.0439\n",
      "Correctly predicted masked tokens: 143 / 4823 (2.965 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 12000/19473 [41:09<25:35,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12000. Epoch: 1. MLM loss: 0.2148. Patience: 3\n",
      "Mean entropy: 4.1731\n",
      "Correctly predicted masked tokens: 117 / 4691 (2.4941 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 12500/19473 [42:52<23:52,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12500. Epoch: 1. MLM loss: 0.2062. Patience: 3\n",
      "Mean entropy: 4.2026\n",
      "Correctly predicted masked tokens: 157 / 4774 (3.2886 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 13000/19473 [44:34<22:09,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 13000. Epoch: 1. MLM loss: 0.1983. Patience: 3\n",
      "Mean entropy: 4.3419\n",
      "Correctly predicted masked tokens: 155 / 4744 (3.2673 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 13500/19473 [46:17<20:28,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 13500. Epoch: 1. MLM loss: 0.1908. Patience: 3\n",
      "Mean entropy: 4.4118\n",
      "Correctly predicted masked tokens: 141 / 4811 (2.9308 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 14000/19473 [47:59<18:46,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 14000. Epoch: 1. MLM loss: 0.1841. Patience: 3\n",
      "Mean entropy: 4.5218\n",
      "Correctly predicted masked tokens: 111 / 4871 (2.2788 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 14500/19473 [49:42<17:03,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 14500. Epoch: 1. MLM loss: 0.1776. Patience: 3\n",
      "Mean entropy: 4.6857\n",
      "Correctly predicted masked tokens: 135 / 4721 (2.8596 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 15000/19473 [51:24<15:21,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 15000. Epoch: 1. MLM loss: 0.1717. Patience: 3\n",
      "Mean entropy: 4.7635\n",
      "Correctly predicted masked tokens: 114 / 4780 (2.3849 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 15500/19473 [53:08<13:43,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 15500. Epoch: 1. MLM loss: 0.1661. Patience: 3\n",
      "Mean entropy: 4.8268\n",
      "Correctly predicted masked tokens: 138 / 4964 (2.78 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 16000/19473 [54:51<12:00,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16000. Epoch: 1. MLM loss: 0.1610. Patience: 3\n",
      "Mean entropy: 4.8371\n",
      "Correctly predicted masked tokens: 164 / 4789 (3.4245 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 16500/19473 [56:34<10:17,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16500. Epoch: 1. MLM loss: 0.1560. Patience: 3\n",
      "Mean entropy: 4.9073\n",
      "Correctly predicted masked tokens: 122 / 4822 (2.5301 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 17000/19473 [58:17<08:32,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 17000. Epoch: 1. MLM loss: 0.1515. Patience: 3\n",
      "Mean entropy: 4.9364\n",
      "Correctly predicted masked tokens: 116 / 4926 (2.3549 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 17500/19473 [1:00:00<06:47,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 17500. Epoch: 1. MLM loss: 0.1471. Patience: 3\n",
      "Mean entropy: 4.9816\n",
      "Correctly predicted masked tokens: 142 / 4848 (2.929 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 18000/19473 [1:01:43<05:03,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18000. Epoch: 1. MLM loss: 0.1430. Patience: 3\n",
      "Mean entropy: 5.0167\n",
      "Correctly predicted masked tokens: 134 / 4762 (2.8139 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 18500/19473 [1:03:26<03:20,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18500. Epoch: 1. MLM loss: 0.1391. Patience: 3\n",
      "Mean entropy: 4.9511\n",
      "Correctly predicted masked tokens: 205 / 4736 (4.3285 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 19000/19473 [1:05:09<01:37,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 19000. Epoch: 1. MLM loss: 0.1354. Patience: 3\n",
      "Mean entropy: 5.014\n",
      "Correctly predicted masked tokens: 145 / 4817 (3.0102 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19473/19473 [1:06:47<00:00,  4.86it/s]\n",
      "  0%|          | 27/19473 [00:05<1:06:52,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 19500. Epoch: 2. MLM loss: 0.1320. Patience: 3\n",
      "Mean entropy: 5.0309\n",
      "Correctly predicted masked tokens: 131 / 4651 (2.8166 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 527/19473 [01:48<1:05:09,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20000. Epoch: 2. MLM loss: 0.1286. Patience: 3\n",
      "Mean entropy: 5.0233\n",
      "Correctly predicted masked tokens: 125 / 4808 (2.5998 %).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 527/19473 [01:48<1:05:13,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed enough steps.\n",
      "Fine-tuning:\n",
      "Number of parameters: 773_042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9196 [00:31<46:19:39, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.348, 0.02801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2502/9196 [09:30<11:48:10,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.91342, 0.7974]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [18:28<10:28:26,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93815, 0.82926]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [32:42<00:00,  4.69it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 T loss: 2.03367. V loss: 1.45920. F1: [0.93885, 0.85342]. P: [0.93636, 0.86178] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9196 [00:29<44:39:10, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.93849, 0.86398]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2502/9196 [09:26<11:46:11,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.94704, 0.87765]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [18:22<10:25:50,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.94981, 0.88286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [32:33<00:00,  4.71it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 T loss: 1.20462. V loss: 1.13109. F1: [0.95273, 0.88658]. P: [0.95174, 0.88691] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9196 [00:29<44:35:34, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.94957, 0.88635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2502/9196 [09:30<11:51:41,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.95439, 0.89453]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [18:25<10:25:18,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.95805, 0.89627]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [33:05<00:00,  4.63it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 T loss: 0.97474. V loss: 1.03495. F1: [0.95888, 0.89874]. P: [0.95798, 0.90307] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/9196 [00:30<45:41:51, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.9548, 0.9002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2500/9196 [09:31<23:06,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halting evaluation after 61056 data points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2502/9196 [10:02<12:18:32,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.96177, 0.90755]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5000/9196 [19:11<16:14,  4.31it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halting evaluation after 53504 data points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5001/9196 [19:42<10:54:32,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.9697, 0.90802]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9196/9196 [35:01<00:00,  4.38it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 T loss: 0.84054. V loss: 0.89632. F1: [0.96534, 0.91211]. P: [0.96508, 0.91451] Patience: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/9196 [00:30<24:32:20,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.96495, 0.91195]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2502/9196 [09:44<11:59:09,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: [0.96592, 0.90785]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2688/9196 [10:22<25:07,  4.32it/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# classifier.model.freeze_base()\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFine-tuning:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m model = \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMAMBA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\n\u001b[32m    130\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m matrix = models.confusion_matrix(model, test_data, \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, mapping)\n\u001b[32m    132\u001b[39m plt.matshow(matrix)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(classifier, name, max_epochs, loss_fn, evaluation_interval, learning_rate)\u001b[39m\n\u001b[32m     54\u001b[39m     optimizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     55\u001b[39m a = time()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m losses, f1, validation_losses = \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_function_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msupervised\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_interval\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m b = time()\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(b\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39ma)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/stelaro/models/__init__.py:781\u001b[39m, in \u001b[36mClassifier.train\u001b[39m\u001b[34m(self, train_loader, validate_loader, optimizer, max_n_epochs, patience, loss_function, loss_function_type, evaluation_interval)\u001b[39m\n\u001b[32m    777\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._compute_loss(\n\u001b[32m    778\u001b[39m     x_batch, y_batch, loss_function_type, loss_function\n\u001b[32m    779\u001b[39m )\n\u001b[32m    780\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clip:\n\u001b[32m    783\u001b[39m     clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), \u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/maitrise/stelaro/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, AdamW\n",
    "import torch\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel, MambaConfig\n",
    "from mamba_ssm import Mamba\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm + residual connection\n",
    "        return x + self.mamba(self.norm(x))\n",
    "\n",
    "\n",
    "class MambaSequenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        N: int,\n",
    "        num_classes: int,\n",
    "        vocab_size: int = 256,\n",
    "        d_model: int = 256,\n",
    "        n_layers: int = 3,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        pooling = \"mean\",\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, d_model)\n",
    "        self.first_layers = nn.ModuleList([\n",
    "            MambaBlock(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.last_layers = nn.ModuleList([\n",
    "            MambaBlock(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Conv1d(128, 256, kernel_size=5, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(N * 256, num_classes),\n",
    "        # )\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.mlm_head = nn.Linear(d_model, vocab_size + 1)\n",
    "        # self.mlm_head.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, x: torch.LongTensor, mlm=None) -> torch.Tensor:\n",
    "        h = self.embedding(x)#.to(dtype=torch.get_default_dtype())\n",
    "        if mlm is not None:\n",
    "            for block in self.first_layers:\n",
    "                h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "            for block in self.last_layers:\n",
    "                h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "            h = self.norm(h)\n",
    "            logits = self.mlm_head(h)  # [B, L, vocab_size + 1]\n",
    "            return logits\n",
    "        else:\n",
    "            for block in self.first_layers:\n",
    "                h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "            for block in self.last_layers:\n",
    "                h = block(h)   # each Mamba block returns [B, L, d_model]\n",
    "            h = self.norm(h)\n",
    "            pooled = h.mean(dim=1)  # [B, d_model]\n",
    "            pooled = self.dropout(pooled)\n",
    "            # pooled = pooled.permute(0, 2, 1)\n",
    "            logits = self.classifier(pooled)  # [B, num_classes]\n",
    "            return logits\n",
    "\n",
    "    def freeze_base(self):\n",
    "        for block in self.first_layers:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def unfreeze_base(self):\n",
    "        for block in self.first_layers:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "\n",
    "classifier = models.Classifier(\n",
    "    LENGTH // 4,\n",
    "    mapping,\n",
    "    \"cuda\",\n",
    "    MambaSequenceClassifier,\n",
    "    format.to_tetramers,\n",
    "    True\n",
    ")\n",
    "\n",
    "print(\"Pretraining:\")\n",
    "optimizer = AdamW(classifier.get_parameters(), lr=0.001)\n",
    "pretraining_data = DataLoader(\n",
    "    models.SyntheticTetramerDataset(PROCESSED_PRETRAINING_DATA, labels=False),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "classifier.pretrain(\n",
    "    pretraining_data,\n",
    "    optimizer,\n",
    "    20_000,\n",
    "    256,\n",
    "    patience=3,\n",
    "    mlm_probability=0.1\n",
    ")\n",
    "# classifier.model.freeze_base()\n",
    "\n",
    "print(\"Fine-tuning:\")\n",
    "model = benchmark(\n",
    "    classifier,\n",
    "    \"MAMBA\",\n",
    "    max_epochs=10,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    evaluation_interval=5000,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "matrix = models.confusion_matrix(model, test_data, \"cuda\", mapping)\n",
    "plt.matshow(matrix)\n",
    "plt.show()\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=500, threshold=np.inf)\n",
    "print(matrix)\n",
    "\n",
    "\n",
    "# 128, 3 / 3, 16, 4, 2, mean, 0.1, clip, pretraining: None\n",
    "# 1/10 T loss: 3.08970. V loss: 1.81286. F1: [0.91764, 0.82083]. P: [0.9162, 0.82467] Patience: 3\n",
    "# 2/10 T loss: 1.49454. V loss: 1.34284. F1: [0.94197, 0.87123]. P: [0.94196, 0.87661] Patience: 3\n",
    "\n",
    "# 128, 3 / 3, 16, 4, 2, mean, 0.1, clip, pretraining: 5k, freeze\n",
    "# 1/10 T loss: 9.88273. V loss: 9.09895. F1: [0.35103, 0.10588]. P: [0.38429, 0.12499]\n",
    "\n",
    "# 128, 2 / 2, 16, 4, 2, mean, 0.1, clip, pretraining: 10k, freeze\n",
    "# 1/10 T loss: 3.26994. V loss: 2.42430. F1: [0.89472, 0.75593]. P: [0.89744, 0.76282] Patience: 3\n",
    "\n",
    "# 128, 2 / 2, 16, 4, 2, mean, 0.1, clip, pretraining: 20k, freeze\n",
    "# 1/10 T loss: 4.59330. V loss: 3.32864. F1: [0.84704, 0.67053]. P: [0.84721, 0.67978] Patience: 3\n",
    "# 2/10 T loss: 2.78251. V loss: 2.51102. F1: [0.88506, 0.74861]. P: [0.88034, 0.75483] Patience: 3\n",
    "\n",
    "# 128, 2 / 2, 16, 4, 2, mean, 0.1, clip, pretraining: 20k, no freeze, span\n",
    "# 1/10 T loss: 3.30888. V loss: 2.14662. F1: [0.90234, 0.79144]. P: [0.90968, 0.79664] Patience: 3\n",
    "# 2/10 T loss: 1.76234. V loss: 1.54726. F1: [0.93, 0.84993]. P: [0.92694, 0.85452] Patience: 3\n",
    "# 3/10 T loss: 1.34409. V loss: 1.35847. F1: [0.93591, 0.86929]. P: [0.93794, 0.87497] Patience: 3\n",
    "\n",
    "# 128, 2 / 2, 16, 4, 2, mean, 0.1, clip, pretraining: 10k, no freeze, span\n",
    "# 1/10 T loss: 3.32805. V loss: 1.91426. F1: [0.91257, 0.81601]. P: [0.91242, 0.81589] Patience: 3\n",
    "# 2/10 T loss: 1.67703. V loss: 1.49529. F1: [0.93111, 0.85618]. P: [0.9346, 0.86045] Patience: 3\n",
    "# 3/10 T loss: 1.33147. V loss: 1.36263. F1: [0.93998, 0.86843]. P: [0.93879, 0.87145] Patience: 3\n",
    "\n",
    "# 128, 2 / 2, 16, 4, 2, mean, 0.1, clip, pretraining: 40k, no freeze, span\n",
    "# 1/10 T loss: 2.66859. V loss: 1.88050. F1: [0.91414, 0.81202]. P: [0.90473, 0.82252] Patience: 3\n",
    "# 2/10 T loss: 1.59724. V loss: 1.41483. F1: [0.94162, 0.86189]. P: [0.94196, 0.86865] Patience: 3\n",
    "# 3/10 T loss: 1.28868. V loss: 1.25275. F1: [0.94469, 0.8754]. P: [0.94138, 0.87864] Patience: 3\n",
    "\n",
    "# 128, 2 / 2, 16, 4, 2, mean, 0.1, clip, pretraining: 20k, no freeze, all masked 15 %\n",
    "# 1/10 T loss: 2.53448. V loss: 1.84904. F1: [0.91666, 0.8147]. P: [0.90713, 0.82688] Patience: 3\n",
    "# 2/10 T loss: 1.52909. V loss: 1.40461. F1: [0.94339, 0.86156]. P: [0.9458, 0.86432] Patience: 3\n",
    "# 3/10 T loss: 1.23919. V loss: 1.23687. F1: [0.95087, 0.8776]. P: [0.95401, 0.87988] Patience: 3\n",
    "\n",
    "# 128, 3 / 3, 16, 4, 2, mean, 0.1, clip, pretraining: 20k, no freeze, all masked 10 %\n",
    "# 1/10 T loss: 2.03367. V loss: 1.45920. F1: [0.93885, 0.85342]. P: [0.93636, 0.86178] Patience: 3\n",
    "# 2/10 T loss: 1.20462. V loss: 1.13109. F1: [0.95273, 0.88658]. P: [0.95174, 0.88691] Patience: 3\n",
    "# 3/10 T loss: 0.97474. V loss: 1.03495. F1: [0.95888, 0.89874]. P: [0.95798, 0.90307] Patience: 3\n",
    "# 4/10 T loss: 0.84054. V loss: 0.89632. F1: [0.96534, 0.91211]. P: [0.96508, 0.91451] Patience: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e088828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.model.state_dict(), \"trained_models/mamba_v1/model.pt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55a64d",
   "metadata": {},
   "source": [
    "# Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39735249",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTAX_TRAIN = BERTAX_DIRECTORY + \"train/\"\n",
    "BERTAX_VALIDATION = BERTAX_DIRECTORY + \"validation/\"\n",
    "BERTAX_TEST = BERTAX_DIRECTORY + \"test/\"\n",
    "\n",
    "from time import time\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "from stelaro import models\n",
    "from stelaro.models import autoencoder, feedforward, transformer\n",
    "\n",
    "LENGTH = 1500\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "with open(BERTAX_DIRECTORY + \"statistics/map.json\", \"r\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "\n",
    "train_data = DataLoader(\n",
    "    models.SyntheticMultiLevelTetramerDataset(\n",
    "        BERTAX_TRAIN,\n",
    "        mapping,\n",
    "        (),\n",
    "        1,\n",
    "        balance = True,\n",
    "        other_factor = 0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "validation_data = DataLoader(\n",
    "    models.SyntheticMultiLevelTetramerDataset(\n",
    "        BERTAX_VALIDATION,\n",
    "        mapping,\n",
    "        (),\n",
    "        1,\n",
    "        balance = False,\n",
    "        other_factor = 0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_data = DataLoader(\n",
    "    models.SyntheticMultiLevelTetramerDataset(\n",
    "        BERTAX_TEST,\n",
    "        mapping,\n",
    "        (),\n",
    "        1,\n",
    "        balance = False,\n",
    "        other_factor = 0\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "mapping = train_data.dataset.mapping\n",
    "\n",
    "\n",
    "def benchmark(\n",
    "        classifier: models.Classifier,\n",
    "        name: str,\n",
    "        max_epochs: int = 20,\n",
    "        learning_rate: float = 0.001\n",
    "    ):\n",
    "    parameters = classifier.get_parameters()\n",
    "    if parameters:\n",
    "        optimizer = Adam(classifier.get_parameters(), lr=learning_rate)\n",
    "        total_params = sum(param.numel() for param in parameters)\n",
    "        print(f\"Number of parameters: {total_params:_}\")\n",
    "    else:\n",
    "        optimizer = None\n",
    "    a = time()\n",
    "    losses, f1, validation_losses = classifier.train(\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        optimizer,\n",
    "        max_n_epochs=max_epochs,\n",
    "        patience=3,\n",
    "        loss_function=nn.CrossEntropyLoss(),\n",
    "        loss_function_type=\"supervised\",\n",
    "        evaluation_interval=5000,\n",
    "    )\n",
    "    b = time()\n",
    "    print(f\"Training took {(b - a):.3f} s.\")\n",
    "    if losses:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        x = list(range(len(losses)))\n",
    "        ax[0].plot(x, losses, label=\"Training\")\n",
    "        ax[0].plot(x, validation_losses, label=\"Validation\")\n",
    "        ax[0].set(xlabel='Epochs', ylabel='Loss')\n",
    "        ax[0].set_title(\"Normalized Loss Against Epochs\")\n",
    "        ax[0].legend()\n",
    "        ax[1].set(xlabel='Epochs', ylabel=\"f1\")\n",
    "        ax[1].set_title(\"F1 Score\")\n",
    "        r = 0\n",
    "        for f in f1:\n",
    "            ax[1].plot(x, f, label=f'Rank {r}')\n",
    "            r += 1\n",
    "        ax[1].legend()\n",
    "        fig.suptitle(f\"Classification Training for {name}\")\n",
    "        plt.show()\n",
    "    result = models.evaluate(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"Test F1 score: {rounded_result}\")\n",
    "\n",
    "    result = models.evaluate_precision(classifier, test_data, \"cuda\", mapping)\n",
    "    rounded_result = [float(f\"{r:.5}\") for r in result]\n",
    "    print(f\"Test precision score: {rounded_result}\")\n",
    "\n",
    "    return classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
