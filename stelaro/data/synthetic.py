"""
    Transform reference genomes files into synthetic read files.

    - Author: Vincent Therrien (therrien.vincent.2@courrier.uqam.ca)
    - Affiliation: Département d'informatique, UQÀM
    - File creation date: June 2025
    - License: MIT
"""

import numpy as np
from random import choices, randint, shuffle
from tqdm import tqdm
import json
from . import read_genome
from torch import tensor
from torch.utils.data import Dataset


NUCLEOTIDE_TO_ONEHOT = {
    'A': 0b1000,
    'C': 0b0100,
    'G': 0b0010,
    'T': 0b0001,
    'N': 0b0000,
}
ONEHOT_TO_NUCLEOTIDE = {v: k for k, v in NUCLEOTIDE_TO_ONEHOT.items()}
SEQUENCE_TO_SAMPLE_MINIMUM_LENGTH_RATIO = 1.5
MAXIMUM_UNDEFINED_FRACTION = 0.05


def encode_tetramer(sequence: str) -> list[int]:
    if any(base not in 'ACGT' for base in sequence):
        raise ValueError("Input must be a 4-character string containing only A, C, G, T.")
    base_to_bits = {
        'A': 0b00,
        'C': 0b01,
        'G': 0b10,
        'T': 0b11,
    }

    def encode_four_nucleotides(tetramer):
        result = 0
        for base in tetramer:
            result = (result << 2) | base_to_bits[base]
        return result

    tetramers = [sequence[i:i+4] for i in range(0, len(sequence), 4)]
    return [encode_four_nucleotides(t) for t in tetramers]


def decode_tetramer(sequence: list[int]) -> str:
    bits_to_base = {
        0b00: 'A',
        0b01: 'C',
        0b10: 'G',
        0b11: 'T'
    }

    def decode_integer(integer):
        tetramer = ''
        for shift in (6, 4, 2, 0):
            two_bits = (integer >> shift) & 0b11
            tetramer += bits_to_base[two_bits]
        return tetramer

    return "".join([decode_integer(i) for i in sequence])


def find_index_from_label(
        index_to_taxonomic_label: dict,
        label: tuple[str]
        ) -> int:
    """Determine the index of a particular taxonomic label."""
    for i in index_to_taxonomic_label:
        if index_to_taxonomic_label[i] == label:
            return i
    raise RuntimeError(f"Label {label} has no index.")


def sample_read(
        genome: list[str],
        n_reads: int,
        length: int,
        ) -> list[int]:
    """Generate synthetic reads from a genome."""
    samples = []
    minimum = SEQUENCE_TO_SAMPLE_MINIMUM_LENGTH_RATIO * length
    indices = [i for i in range(len(genome)) if len(genome[i][1]) > minimum]
    if len(indices) < 1:
        raise RuntimeError(f"Reads are too short.")
    sequence_lengths = []
    for i in indices:
        sequence_lengths.append(len(genome[i][1]))
    for _ in range(n_reads):
        for _ in range(5):
            selection = choices(indices, weights=sequence_lengths, k=1)[0]
            sequence = genome[selection][1]
            cursor = randint(0, len(sequence) - length)
            sequence = sequence[cursor:cursor + length]
            fraction = sequence.count('N') / length
            if fraction < MAXIMUM_UNDEFINED_FRACTION:
                encoding = [NUCLEOTIDE_TO_ONEHOT[s] for s in sequence]
                samples.append(encoding)
                break
        else:
            raise RuntimeError(f"Could not generate an acceptable read.")
    return samples


def decode(sequence: list[int]) -> str:
    characters = [ONEHOT_TO_NUCLEOTIDE[s] for s in sequence]
    return "".join(characters)


def write(
        dataset: list,
        index_to_taxonomic_label: dict,
        index_to_n_passes: dict,
        reference_genome_directory: str,
        read_length: int,
        dst: str,
        ) -> int:
    """Write a synthetic read dataset in a Numpy array.

    The `x` array contains one-hot encoded reads. The `y` arrays contains an
    identifier for the origin of the corresponding read.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        index_to_n_passes: Number of times that the genomes in each bin must be
            sampled.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.
        dst: Path of the file in which to write the result.

    Returns: Number of sampled synthetic reads.
    """
    # Preparation
    assert reference_genome_directory.endswith("/")
    n_reads = 0
    for label, genome_list in dataset:
        index = find_index_from_label(index_to_taxonomic_label, label)
        n_passes = index_to_n_passes[index]
        for _, sub_genome_list in genome_list:
            n_reads += len(sub_genome_list) * n_passes
    size = n_reads * read_length
    print(f"Estimated file size: {size / 1000000} MB.")
    print(f"Average number of reads per bin: {n_reads / len(dataset):.2f}")
    # Create the datasets.
    i = 0
    x = np.zeros((n_reads, read_length), dtype=np.int8)
    y = np.zeros(n_reads, dtype=np.int16)
    with tqdm(total=n_reads) as progress_bar:
        for label, genome_list in dataset:
            index = find_index_from_label(index_to_taxonomic_label, label)
            n_passes = index_to_n_passes[index]
            for _, sub_genome_list in genome_list:
                for genome_id in sub_genome_list:
                    filename = f"{reference_genome_directory}{genome_id}.fna"
                    genome = read_genome(filename)
                    try:
                        reads = sample_read(genome, n_passes, read_length)
                    except Exception as e:
                        print(f"Error on {genome_id}: {e}")
                    for read in reads:
                        x[i] = read
                        y[i] = index
                        i += 1
                        progress_bar.update(1)
    np.save(f"{dst}x.npy", x)
    np.save(f"{dst}y.npy", y)
    with open(f"{dst}map.json", "w") as f:
        json.dump(index_to_taxonomic_label, f, indent=4)
    return i


def sample_dataset(
        dataset: list,
        index_to_taxonomic_label: dict,
        reference_genome_directory: str,
        read_length: int,
        n_reads: int,
        distribution: dict,
        ) -> tuple[np.ndarray]:
    """Write a synthetic read dataset in a Numpy array.

    The `x` array contains one-hot encoded reads. The `y` arrays contains an
    identifier for the origin of the corresponding read.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.
        n_reads: Number of reads to sample.
        distribution: Map a taxonomic index to a sampling frequency. Values
            must average 1.

    Returns: The `x` and `y` datasets.
    """
    # Preparation
    assert reference_genome_directory.endswith("/")
    assert abs(np.mean(list(distribution.values())) - 1.0) < 0.01
    size = n_reads * read_length
    genome_counts = {}
    n_genomes = 0
    for taxon in dataset:
        label, genome_list = taxon
        index = find_index_from_label(index_to_taxonomic_label, label)
        genome_counts[index] = 0
        for reference_genome in genome_list:
            _, genomes = reference_genome
            genome_counts[index] += len(genomes)
            n_genomes += len(genomes)
    print(f"Estimated size: {size / 1000000} MB.")
    print(f"Average number of reads per bin: {n_reads / len(dataset):.2f}")
    # Create the datasets.
    i = 0
    x = np.zeros((n_reads, read_length), dtype=np.int8)
    y = np.zeros(n_reads, dtype=np.int16)
    with tqdm(total=n_reads) as progress_bar:
        for label, genome_list in dataset:
            index = find_index_from_label(index_to_taxonomic_label, label)
            n_taxon_genomes = genome_counts[index]
            fraction = n_taxon_genomes / n_genomes
            n_to_sample = int(n_reads * fraction * distribution[index])
            j = 0
            while j < n_to_sample and i < n_reads:
                for _, sub_genome_list in genome_list:
                    for genome_id in sub_genome_list:
                        filename = f"{reference_genome_directory}{genome_id}.fna"
                        genome = read_genome(filename)
                        try:
                            reads = sample_read(genome, 1, read_length)
                        except Exception as e:
                            pass  # print(f"Error on {genome_id}: {e}")
                        for read in reads:
                            x[i] = read
                            y[i] = index
                            i += 1
                            j += 1
                            progress_bar.update(1)
                        if j > n_to_sample:
                            break
                        if i >= n_reads:
                            return x, y
                    if j > n_to_sample:
                        break
    return x, y


def evaluate_n_nucleotides(
        references: tuple[str],
        reference_genome_directory: str,
        ) -> int:
    """Obtain the number of nucleotides in a set of genomes."""
    assert reference_genome_directory.endswith("/")
    length = 0
    for reference in references:
        filename = f"{reference_genome_directory}{reference}.fna"
        genome = read_genome(filename)
        for fragment in genome:
            name, sequence = fragment
            length += len(sequence)
    return length


def compress_dataset(
        dataset: list,
        index_to_taxonomic_label: dict,
        reference_genome_directory: str,
        read_length: int,
        output_directory: str,
        max_n_reads: int = 1_000_000
        ) -> None:
    """Compress sequencing files into Numpy arrays.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.

    Returns: The `x` and `y` datasets.
    """
    assert read_length / 4 == int(read_length / 4)
    assert output_directory.endswith("/")
    def initialize_datasets():
        x = np.zeros((max_n_reads, int(read_length / 4)), dtype=np.uint8)
        y = np.zeros(max_n_reads, dtype=np.uint16)
        return x, y

    def save(x, y, file_number):
        np.save(output_directory + str(file_number) + "_x.npy", x)
        np.save(output_directory + str(file_number) + "_y.npy", y)

    x, y = initialize_datasets()
    array_index = 0
    file_number = 0
    counts = {0: 0}
    for label, genome_list in tqdm(dataset):
        index = find_index_from_label(index_to_taxonomic_label, label)
        for _, sub_genome_list in genome_list:
            for genome_id in sub_genome_list:
                filename = f"{reference_genome_directory}{genome_id}.fna"
                genome = read_genome(filename)
                for fragment in genome:
                    name, sequence = fragment
                    reads = [
                        sequence[i:i+read_length] for i in range(
                            0, len(sequence), read_length
                        )
                    ]
                    for read in reads:
                        if len(read) != read_length:
                            continue
                        try:
                            encoding = encode_tetramer(read)
                            x[array_index] = encoding
                            y[array_index] = index
                            array_index += 1
                            counts[file_number] += 1
                            if array_index >= max_n_reads:
                                save(x, y, file_number)
                                x, y = initialize_datasets()
                                file_number += 1
                                counts[file_number] = 0
                                array_index = 0
                        except:
                            pass
    save(x, y, file_number)
    with open(output_directory + "counts.json", "w") as file:
        json.dump(counts, file, indent=4)


def get_n_reads_in_compressed_dataset(
        directory: str,
    ) -> tuple[np.ndarray]:
    """Get the number of reads in a compressed dataset."""
    assert directory.endswith("/")
    with open(directory + "counts.json", "r") as file:
        counts = json.load(file)
    y = None
    for identifier, count in counts.items():
        partial_y = np.load(directory + str(identifier) + "_y.npy")
        partial_y = partial_y[:count]
        if y is None:
            y = partial_y
        else:
            y = np.concatenate((y, partial_y))
    frequencies = np.bincount(y)
    return sum(frequencies)


def get_random_identifiers(n: int) -> dict:
    """Create a dictionary that maps an index to a unique random index."""
    identifiers = list(range(n))
    shuffle(identifiers)
    return {i: r for i, r in zip(range(n), identifiers)}


def sample_compressed_dataset(
        directory: str,
        n: int,
        read_length: int,
        identifiers: dict,
        offset: int,
        ) -> tuple[np.ndarray]:
    """Obtain reads from a compressed dataset.

    Args:
        directory: Directory that contains the compressed data.
        n: Number of reads to sample.
        read_length: Length of the reads.
        identifiers: Map generated by the function `get_random_identifiers`.
        offset: Index at which to start reading identifiers.
    """
    assert directory.endswith("/")
    x = np.zeros((n, int(read_length / 4)), dtype=np.uint8)
    y = np.zeros(n, dtype=np.uint16)
    with open(directory + "counts.json", "r") as file:
        counts = json.load(file)
    global_offset = 0
    for identifier, count in counts.items():
        partial_x = np.load(directory + str(identifier) + "_x.npy")
        partial_x = partial_x[:count]
        partial_y = np.load(directory + str(identifier) + "_y.npy")
        partial_y = partial_y[:count]
        for i, global_src_index in identifiers.items():
            if i < offset:
                continue
            if i >= offset + n:
                break
            local_src_index = global_src_index - global_offset
            if 0 <= local_src_index < count:
                x[i - offset] = partial_x[local_src_index]
                y[i - offset] = partial_y[local_src_index]
        global_offset += count
    return x, y


class SyntheticReadDataset(Dataset):
    def __init__(self, directory: str, n: int, read_length: int):
        ids = get_random_identifiers(n)
        self.n = n
        self.x, self.y = sample_compressed_dataset(
            directory,
            n,
            read_length,
            ids,
            0
        )

    def __len__(self):
        return self.n

    def __getitem__(self, idx):
        x = self.x[idx]
        y = self.y[idx]
        return tensor(x), tensor(y).to(int)


class CompressedReadDataset(Dataset):
    """Dataset containing 4mer-encoded synthetic reads split in multiple files.

    The directory containing the reads is expected to contain the files:
    - `<x>_x.npy`, where `<x>` is an integer.
    - `<y>_y.npy`, where `<y>` is an integer.
    - `counts.json`

    Data are loaded from disk into main memory.
    """
    def __init__(
            self,
            directory: str,
            partition_size: int,
            ):
        """Args:
            directory: Path of the directory that contains the x and y files.
            fragment: Number of reads in a partition of the dataset.
        """
        self.directory = directory
        self.offset = 0
        self.partition_size = partition_size
        self.n = get_n_reads_in_compressed_dataset(directory)
        print(self.n)
        self.identifiers = get_random_identifiers(self.n)
        probe = np.load(directory + "0_x.npy")
        self.read_length = probe.shape[1] * 4
        self.load_partition()

    def load_partition(self):
        if self.offset + self.partition_size >= self.n:
            partition_size = self.n - self.offset
        else:
            partition_size = self.partition_size
        self.x, self.y = sample_compressed_dataset(
            self.directory,
            partition_size,
            self.read_length,
            self.identifiers,
            self.offset
        )
        self.offset += self.partition_size
        if self.offset >= self.n:
            self.offset = 0

    def __len__(self):
        return self.n

    def __getitem__(self, idx):
        local_index = idx - self.offset
        if local_index >= self.partition_size:
            self.load_partition()
            local_index = 0
        return tensor(self.x[local_index]), tensor(self.y[local_index]).to(int)
