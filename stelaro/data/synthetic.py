"""
    Transform reference genomes files into synthetic read files.

    - Author: Vincent Therrien (therrien.vincent.2@courrier.uqam.ca)
    - Affiliation: Département d'informatique, UQÀM
    - File creation date: June 2025
    - License: MIT
"""

import numpy as np
from random import choices, randint
from tqdm import tqdm
import json
from . import read_genome


NUCLEOTIDE_TO_ONEHOT = {
    'A': 0b1000,
    'C': 0b0100,
    'G': 0b0010,
    'T': 0b0001,
    'N': 0b0000,
}
ONEHOT_TO_NUCLEOTIDE = {v: k for k, v in NUCLEOTIDE_TO_ONEHOT.items()}
SEQUENCE_TO_SAMPLE_MINIMUM_LENGTH_RATIO = 1.5
MAXIMUM_UNDEFINED_FRACTION = 0.05


def encode_tetramer(sequence: str) -> list[int]:
    if any(base not in 'ACGT' for base in sequence):
        raise ValueError("Input must be a 4-character string containing only A, C, G, T.")
    base_to_bits = {
        'A': 0b00,
        'C': 0b01,
        'G': 0b10,
        'T': 0b11,
    }

    def encode_four_nucleotides(tetramer):
        result = 0
        for base in tetramer:
            result = (result << 2) | base_to_bits[base]
        return result

    tetramers = [sequence[i:i+4] for i in range(0, len(sequence), 4)]
    return [encode_four_nucleotides(t) for t in tetramers]


def decode_tetramer(sequence: list[int]) -> str:
    bits_to_base = {
        0b00: 'A',
        0b01: 'C',
        0b10: 'G',
        0b11: 'T'
    }

    def decode_integer(integer):
        tetramer = ''
        for shift in (6, 4, 2, 0):
            two_bits = (integer >> shift) & 0b11
            tetramer += bits_to_base[two_bits]
        return tetramer

    return "".join([decode_integer(i) for i in sequence])


def find_index_from_label(
        index_to_taxonomic_label: dict,
        label: tuple[str]
        ) -> int:
    """Determine the index of a particular taxonomic label."""
    for i in index_to_taxonomic_label:
        if index_to_taxonomic_label[i] == label:
            return i
    raise RuntimeError(f"Label {label} has no index.")


def sample_read(
        genome: list[str],
        n_reads: int,
        length: int,
        ) -> list[int]:
    """Generate synthetic reads from a genome."""
    samples = []
    minimum = SEQUENCE_TO_SAMPLE_MINIMUM_LENGTH_RATIO * length
    indices = [i for i in range(len(genome)) if len(genome[i][1]) > minimum]
    if len(indices) < 1:
        raise RuntimeError(f"Reads are too short.")
    sequence_lengths = []
    for i in indices:
        sequence_lengths.append(len(genome[i][1]))
    for _ in range(n_reads):
        for _ in range(5):
            selection = choices(indices, weights=sequence_lengths, k=1)[0]
            sequence = genome[selection][1]
            cursor = randint(0, len(sequence) - length)
            sequence = sequence[cursor:cursor + length]
            fraction = sequence.count('N') / length
            if fraction < MAXIMUM_UNDEFINED_FRACTION:
                encoding = [NUCLEOTIDE_TO_ONEHOT[s] for s in sequence]
                samples.append(encoding)
                break
        else:
            raise RuntimeError(f"Could not generate an acceptable read.")
    return samples


def decode(sequence: list[int]) -> str:
    characters = [ONEHOT_TO_NUCLEOTIDE[s] for s in sequence]
    return "".join(characters)


def write(
        dataset: list,
        index_to_taxonomic_label: dict,
        index_to_n_passes: dict,
        reference_genome_directory: str,
        read_length: int,
        dst: str,
        ) -> int:
    """Write a synthetic read dataset in a Numpy array.

    The `x` array contains one-hot encoded reads. The `y` arrays contains an
    identifier for the origin of the corresponding read.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        index_to_n_passes: Number of times that the genomes in each bin must be
            sampled.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.
        dst: Path of the file in which to write the result.

    Returns: Number of sampled synthetic reads.
    """
    # Preparation
    assert reference_genome_directory.endswith("/")
    n_reads = 0
    for label, genome_list in dataset:
        index = find_index_from_label(index_to_taxonomic_label, label)
        n_passes = index_to_n_passes[index]
        for _, sub_genome_list in genome_list:
            n_reads += len(sub_genome_list) * n_passes
    size = n_reads * read_length
    print(f"Estimated file size: {size / 1000000} MB.")
    print(f"Average number of reads per bin: {n_reads / len(dataset):.2f}")
    # Create the datasets.
    i = 0
    x = np.zeros((n_reads, read_length), dtype=np.int8)
    y = np.zeros(n_reads, dtype=np.int16)
    with tqdm(total=n_reads) as progress_bar:
        for label, genome_list in dataset:
            index = find_index_from_label(index_to_taxonomic_label, label)
            n_passes = index_to_n_passes[index]
            for _, sub_genome_list in genome_list:
                for genome_id in sub_genome_list:
                    filename = f"{reference_genome_directory}{genome_id}.fna"
                    genome = read_genome(filename)
                    try:
                        reads = sample_read(genome, n_passes, read_length)
                    except Exception as e:
                        print(f"Error on {genome_id}: {e}")
                    for read in reads:
                        x[i] = read
                        y[i] = index
                        i += 1
                        progress_bar.update(1)
    np.save(f"{dst}x.npy", x)
    np.save(f"{dst}y.npy", y)
    with open(f"{dst}map.json", "w") as f:
        json.dump(index_to_taxonomic_label, f, indent=4)
    return i


def sample_dataset(
        dataset: list,
        index_to_taxonomic_label: dict,
        reference_genome_directory: str,
        read_length: int,
        n_reads: int,
        distribution: dict,
        ) -> tuple[np.ndarray]:
    """Write a synthetic read dataset in a Numpy array.

    The `x` array contains one-hot encoded reads. The `y` arrays contains an
    identifier for the origin of the corresponding read.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.
        n_reads: Number of reads to sample.
        distribution: Map a taxonomic index to a sampling frequency. Values
            must average 1.

    Returns: The `x` and `y` datasets.
    """
    # Preparation
    assert reference_genome_directory.endswith("/")
    assert abs(np.mean(list(distribution.values())) - 1.0) < 0.01
    size = n_reads * read_length
    genome_counts = {}
    n_genomes = 0
    for taxon in dataset:
        label, genome_list = taxon
        index = find_index_from_label(index_to_taxonomic_label, label)
        genome_counts[index] = 0
        for reference_genome in genome_list:
            _, genomes = reference_genome
            genome_counts[index] += len(genomes)
            n_genomes += len(genomes)
    print(f"Estimated size: {size / 1000000} MB.")
    print(f"Average number of reads per bin: {n_reads / len(dataset):.2f}")
    # Create the datasets.
    i = 0
    x = np.zeros((n_reads, read_length), dtype=np.int8)
    y = np.zeros(n_reads, dtype=np.int16)
    with tqdm(total=n_reads) as progress_bar:
        for label, genome_list in dataset:
            index = find_index_from_label(index_to_taxonomic_label, label)
            n_taxon_genomes = genome_counts[index]
            fraction = n_taxon_genomes / n_genomes
            n_to_sample = int(n_reads * fraction * distribution[index])
            j = 0
            while j < n_to_sample and i < n_reads:
                for _, sub_genome_list in genome_list:
                    for genome_id in sub_genome_list:
                        filename = f"{reference_genome_directory}{genome_id}.fna"
                        genome = read_genome(filename)
                        try:
                            reads = sample_read(genome, 1, read_length)
                        except Exception as e:
                            pass  # print(f"Error on {genome_id}: {e}")
                        for read in reads:
                            x[i] = read
                            y[i] = index
                            i += 1
                            j += 1
                            progress_bar.update(1)
                        if j > n_to_sample:
                            break
                        if i >= n_reads:
                            return x, y
                    if j > n_to_sample:
                        break
    return x, y


def compress_dataset(
        dataset: list,
        index_to_taxonomic_label: dict,
        reference_genome_directory: str,
        read_length: int,
        output_directory: str,
        max_n_reads: int = 1_000_000
        ) -> tuple[np.ndarray]:
    """Compress sequencing files into Numpy arrays.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.

    Returns: The `x` and `y` datasets.
    """
    assert read_length / 4 == int(read_length / 4)
    assert output_directory.endswith("/")
    def initialize_datasets():
        x = np.zeros((max_n_reads, int(read_length / 4)), dtype=np.uint8)
        y = np.zeros(max_n_reads, dtype=np.uint16)
        return x, y

    def save(x, y, file_number):
        np.save(output_directory + str(file_number) + "_x.npy", x)
        np.save(output_directory + str(file_number) + "_y.npy", y)

    x, y = initialize_datasets()
    array_index = 0
    file_number = 0
    counts = {0: 0}
    for label, genome_list in tqdm(dataset):
        index = find_index_from_label(index_to_taxonomic_label, label)
        for _, sub_genome_list in genome_list:
            for genome_id in sub_genome_list:
                filename = f"{reference_genome_directory}{genome_id}.fna"
                genome = read_genome(filename)
                for fragment in genome:
                    name, sequence = fragment
                    reads = [
                        sequence[i:i+read_length] for i in range(
                            0, len(sequence), read_length
                        )
                    ]
                    for read in reads:
                        if len(read) != read_length:
                            continue
                        try:
                            encoding = encode_tetramer(read)
                            x[array_index] = encoding
                            y[array_index] = index
                            array_index += 1
                            counts[file_number] += 1
                            if array_index >= max_n_reads:
                                save(x, y, file_number)
                                x, y = initialize_datasets()
                                file_number += 1
                                counts[file_number] = 0
                                array_index = 0
                        except:
                            pass
    save(x, y, file_number)
    with open(output_directory + "counts.json", "w") as file:
        json.dump(counts, file, indent=4)
