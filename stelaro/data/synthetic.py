"""
    Transform reference genomes files into synthetic read files.

    - Author: Vincent Therrien (therrien.vincent.2@courrier.uqam.ca)
    - Affiliation: Département d'informatique, UQÀM
    - File creation date: June 2025
    - License: MIT
"""

import numpy as np
from random import choices, randint, shuffle
from tqdm import tqdm
import json
from . import read_genome
from torch import tensor
from torch.utils.data import Dataset

NUCLEOTIDE_TO_ONEHOT = {
    'A': 0b1000,
    'C': 0b0100,
    'G': 0b0010,
    'T': 0b0001,
    'N': 0b0000,
}
ONEHOT_TO_NUCLEOTIDE = {v: k for k, v in NUCLEOTIDE_TO_ONEHOT.items()}
SEQUENCE_TO_SAMPLE_MINIMUM_LENGTH_RATIO = 1.5
MAXIMUM_UNDEFINED_FRACTION = 0.05


def find_index_from_label(
        index_to_taxonomic_label: dict,
        label: tuple[str]
        ) -> int:
    """Determine the index of a particular taxonomic label."""
    for i in index_to_taxonomic_label:
        if index_to_taxonomic_label[i] == label:
            return i
    raise RuntimeError(f"Label {label} has no index.")


def sample_read(
        genome: list[str],
        n_reads: int,
        length: int,
        ) -> list[int]:
    """Generate synthetic reads from a genome."""
    samples = []
    minimum = SEQUENCE_TO_SAMPLE_MINIMUM_LENGTH_RATIO * length
    indices = [i for i in range(len(genome)) if len(genome[i][1]) > minimum]
    if len(indices) < 1:
        raise RuntimeError(f"Reads are too short.")
    sequence_lengths = []
    for i in indices:
        sequence_lengths.append(len(genome[i][1]))
    for _ in range(n_reads):
        for _ in range(5):
            selection = choices(indices, weights=sequence_lengths, k=1)[0]
            sequence = genome[selection][1]
            cursor = randint(0, len(sequence) - length)
            sequence = sequence[cursor:cursor + length]
            fraction = sequence.count('N') / length
            if fraction < MAXIMUM_UNDEFINED_FRACTION:
                encoding = [NUCLEOTIDE_TO_ONEHOT[s] for s in sequence]
                samples.append(encoding)
                break
        else:
            raise RuntimeError(f"Could not generate an acceptable read.")
    return samples


def write(
        dataset: list,
        index_to_taxonomic_label: dict,
        index_to_n_passes: dict,
        reference_genome_directory: str,
        read_length: int,
        dst: str,
        ) -> int:
    """Write a synthetic read dataset in a Numpy array.

    The `x` array contains one-hot encoded reads. The `y` arrays contains an
    identifier for the origin of the corresponding read.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        index_to_n_passes: Number of times that the genomes in each bin must be
            sampled.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.
        dst: Path of the file in which to write the result.

    Returns: Number of sampled synthetic reads.
    """
    # Preparation
    assert reference_genome_directory.endswith("/")
    n_reads = 0
    for label, genome_list in dataset:
        index = find_index_from_label(index_to_taxonomic_label, label)
        n_passes = index_to_n_passes[index]
        for _, sub_genome_list in genome_list:
            n_reads += len(sub_genome_list) * n_passes
    size = n_reads * read_length
    print(f"Estimated file size: {size / 1000000} MB.")
    print(f"Average number of reads per bin: {n_reads / len(dataset):.2f}")
    # Create the datasets.
    i = 0
    x = np.zeros((n_reads, read_length), dtype=np.int8)
    y = np.zeros(n_reads, dtype=np.int16)
    with tqdm(total=n_reads) as progress_bar:
        for label, genome_list in dataset:
            index = find_index_from_label(index_to_taxonomic_label, label)
            n_passes = index_to_n_passes[index]
            for _, sub_genome_list in genome_list:
                for genome_id in sub_genome_list:
                    filename = f"{reference_genome_directory}{genome_id}.fna"
                    genome = read_genome(filename)
                    try:
                        reads = sample_read(genome, n_passes, read_length)
                    except Exception as e:
                        print(f"Error on {genome_id}: {e}")
                    for read in reads:
                        x[i] = read
                        y[i] = index
                        i += 1
                        progress_bar.update(1)
    np.save(f"{dst}x.npy", x)
    np.save(f"{dst}y.npy", y)
    with open(f"{dst}map.json", "w") as f:
        json.dump(index_to_taxonomic_label, f, indent=4)
    return i


def sample_dataset(
        dataset: list,
        index_to_taxonomic_label: dict,
        reference_genome_directory: str,
        read_length: int,
        n_reads: int,
        distribution: dict,
        ) -> tuple[np.ndarray]:
    """Write a synthetic read dataset in a Numpy array.

    The `x` array contains one-hot encoded reads. The `y` arrays contains an
    identifier for the origin of the corresponding read.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.
        n_reads: Number of reads to sample.
        distribution: Map a taxonomic index to a sampling frequency. Values
            must average 1.

    Returns: The `x` and `y` datasets.
    """
    # Preparation
    assert reference_genome_directory.endswith("/")
    assert abs(np.mean(list(distribution.values())) - 1.0) < 0.01
    size = n_reads * read_length
    genome_counts = {}
    n_genomes = 0
    for taxon in dataset:
        label, genome_list = taxon
        index = find_index_from_label(index_to_taxonomic_label, label)
        genome_counts[index] = 0
        for reference_genome in genome_list:
            _, genomes = reference_genome
            genome_counts[index] += len(genomes)
            n_genomes += len(genomes)
    print(f"Estimated size: {size / 1000000} MB.")
    print(f"Average number of reads per bin: {n_reads / len(dataset):.2f}")
    # Create the datasets.
    i = 0
    x = np.zeros((n_reads, read_length), dtype=np.int8)
    y = np.zeros(n_reads, dtype=np.int16)
    with tqdm(total=n_reads) as progress_bar:
        for label, genome_list in dataset:
            index = find_index_from_label(index_to_taxonomic_label, label)
            n_taxon_genomes = genome_counts[index]
            fraction = n_taxon_genomes / n_genomes
            n_to_sample = int(n_reads * fraction * distribution[index])
            j = 0
            while j < n_to_sample and i < n_reads:
                for _, sub_genome_list in genome_list:
                    for genome_id in sub_genome_list:
                        filename = f"{reference_genome_directory}{genome_id}.fna"
                        genome = read_genome(filename)
                        try:
                            reads = sample_read(genome, 1, read_length)
                        except Exception as e:
                            pass  # print(f"Error on {genome_id}: {e}")
                        for read in reads:
                            x[i] = read
                            y[i] = index
                            i += 1
                            j += 1
                            progress_bar.update(1)
                        if j > n_to_sample:
                            break
                        if i >= n_reads:
                            return x, y
                    if j > n_to_sample:
                        break
    return x, y


def evaluate_n_nucleotides(
        references: tuple[str],
        reference_genome_directory: str,
        ) -> int:
    """Obtain the number of nucleotides in a set of genomes."""
    assert reference_genome_directory.endswith("/")
    length = 0
    for reference in references:
        filename = f"{reference_genome_directory}{reference}.fna"
        genome = read_genome(filename)
        for fragment in genome:
            name, sequence = fragment
            length += len(sequence)
    return length


def compress_dataset(
        dataset: list,
        index_to_taxonomic_label: dict,
        reference_genome_directory: str,
        read_length: int,
        output_directory: str,
        max_n_reads: int = 1_000_000
        ) -> None:
    """Compress sequencing files into Numpy arrays.

    Args:
        dataset: Maps bins to reference genomes. Generated by
            `ncbi.bin_genomes` or `Taxonomy.bin_genomes`.
        index_to_taxonomic_label: Maps bins to an integer identifier.
        reference_genome_directory: Path of the directory that contains the
            installed reference genomes.
        read_length: Length of the synthetic reads to sample.

    Returns: The `x` and `y` datasets.
    """
    assert read_length / 4 == int(read_length / 4)
    assert output_directory.endswith("/")
    def initialize_datasets():
        x = np.zeros((max_n_reads, int(read_length / 4)), dtype=np.uint8)
        y = np.zeros(max_n_reads, dtype=np.uint16)
        return x, y

    def save(x, y, file_number):
        np.save(output_directory + str(file_number) + "_x.npy", x)
        np.save(output_directory + str(file_number) + "_y.npy", y)

    x, y = initialize_datasets()
    array_index = 0
    file_number = 0
    counts = {0: 0}
    for label, genome_list in tqdm(dataset):
        index = find_index_from_label(index_to_taxonomic_label, label)
        for _, sub_genome_list in genome_list:
            for genome_id in sub_genome_list:
                filename = f"{reference_genome_directory}{genome_id}.fna"
                genome = read_genome(filename)
                for fragment in genome:
                    name, sequence = fragment
                    reads = [
                        sequence[i:i+read_length] for i in range(
                            0, len(sequence), read_length
                        )
                    ]
                    for read in reads:
                        if len(read) != read_length:
                            continue
                        try:
                            encoding = format.encode_tetramer(read)
                            x[array_index] = encoding
                            y[array_index] = index
                            array_index += 1
                            counts[file_number] += 1
                            if array_index >= max_n_reads:
                                save(x, y, file_number)
                                x, y = initialize_datasets()
                                file_number += 1
                                counts[file_number] = 0
                                array_index = 0
                        except:
                            pass
    save(x, y, file_number)
    with open(output_directory + "counts.json", "w") as file:
        json.dump(counts, file, indent=4)


def get_n_reads_in_compressed_dataset(
        directory: str,
    ) -> tuple[np.ndarray]:
    """Get the number of reads in a compressed dataset."""
    assert directory.endswith("/")
    with open(directory + "counts.json", "r") as file:
        counts = json.load(file)
    y = None
    for identifier, count in counts.items():
        partial_y = np.load(directory + str(identifier) + "_y.npy")
        partial_y = partial_y[:count]
        if y is None:
            y = partial_y
        else:
            y = np.concatenate((y, partial_y))
    return np.bincount(y)


def get_random_identifiers(n: int) -> dict:
    """Create a dictionary that maps an index to a unique random index."""
    identifiers = list(range(n))
    shuffle(identifiers)
    return {i: r for i, r in zip(range(n), identifiers)}


def get_floored_random_identifiers(directory: str, floor: int, n: int) -> dict:
    """Same as `get_random_identifiers`, but generates identifiers that
    include all elements of a class if its count is low.
    """
    assert floor > 1, "Invalid floor."
    # Determine the number of samples for each class.
    total_reads = get_n_reads_in_compressed_dataset(directory)
    counts_to_fetch = {}
    for i, n_reads in enumerate(total_reads):
        if n_reads < floor:
            counts_to_fetch[i] = int(n_reads)
        else:
            counts_to_fetch[i] = 0
    n_remaining = n - sum(counts_to_fetch.values())
    n_remaining_classes = len([0 for _, v in counts_to_fetch.items() if v == 0])
    average = n_remaining / n_remaining_classes
    for i, n_reads in enumerate(total_reads):
        if counts_to_fetch[i] == 0:
            counts_to_fetch[i] = int(average)
            if average > n_reads:
                raise RuntimeError(
                    "Insufficient number of reads. "
                    + f"Required: {average}. Available: {n_reads}."
                )
    # Obtain sample indices.
    assert directory.endswith("/")
    with open(directory + "counts.json", "r") as file:
        counts = json.load(file)
    identifiers = {k: [] for k in counts_to_fetch}
    global_index = 0
    for identifier, count in counts.items():
        partial_y = np.load(directory + str(identifier) + "_y.npy")
        partial_y = partial_y[:count]
        for i in range(len(partial_y)):
            identifiers[partial_y[i]].append(global_index)
            global_index += 1
    # Reduce the number of samples.
    indices = []
    for i in range(len(identifiers)):
        shuffle(identifiers[i])
        indices += identifiers[i][:counts_to_fetch[i]]
    shuffle(indices)
    identifiers = {}
    for i, v in enumerate(indices):
        identifiers[i] = v
    return identifiers


def get_identifiers_by_class(directory: str) -> dict:
    total_reads = get_n_reads_in_compressed_dataset(directory)
    counts_to_fetch = {}
    for i, n_reads in enumerate(total_reads):
        counts_to_fetch[i] = int(n_reads)
    assert directory.endswith("/")
    with open(directory + "counts.json", "r") as file:
        counts = json.load(file)
    identifiers = {k: [] for k in counts_to_fetch}
    global_index = 0
    for identifier, count in counts.items():
        partial_y = np.load(directory + str(identifier) + "_y.npy")
        partial_y = partial_y[:count]
        for i in range(len(partial_y)):
            identifiers[partial_y[i]].append(global_index)
            global_index += 1
    return identifiers


def get_balanced_random_identifiers(directory: str, floor: int) -> dict:
    """TODO"""
    total_reads = get_n_reads_in_compressed_dataset(directory)
    counts_to_fetch = {}
    for i, n_reads in enumerate(total_reads):
        if n_reads < floor:
            counts_to_fetch[i] = int(n_reads)
        else:
            counts_to_fetch[i] = 0


def sample_compressed_dataset(
        directory: str,
        n: int,
        read_length: int,
        identifiers: dict,
        offset: int,
        ) -> tuple[np.ndarray]:
    """Obtain reads from a compressed dataset.

    Args:
        directory: Directory that contains the compressed data.
        n: Number of reads to sample.
        read_length: Length of the reads.
        identifiers: Map generated by the function `get_random_identifiers`.
        offset: Index at which to start reading identifiers.
    """
    assert directory.endswith("/")
    x = np.zeros((n, int(read_length / 4)), dtype=np.uint8)
    y = np.zeros(n, dtype=np.uint16)
    with open(directory + "counts.json", "r") as file:
        counts = json.load(file)
    global_offset = 0
    for identifier, count in counts.items():
        partial_x = np.load(directory + str(identifier) + "_x.npy")
        partial_x = partial_x[:count]
        partial_y = np.load(directory + str(identifier) + "_y.npy")
        partial_y = partial_y[:count]
        for i, global_src_index in identifiers.items():
            if i < offset:
                continue
            if i >= offset + n:
                break
            local_src_index = global_src_index - global_offset
            if 0 <= local_src_index < count:
                x[i - offset] = partial_x[local_src_index]
                y[i - offset] = partial_y[local_src_index]
        global_offset += count
    return x, y


class SyntheticReadDataset(Dataset):
    """Dataset containing 4mer-encoded synthetic reads stored in multiple
    files.

    The directory containing the reads is expected to contain the files:
    - `<x>_x.npy`, where `<x>` is an integer.
    - `<y>_y.npy`, where `<y>` is an integer.
    - `counts.json`

    Data are fully loaded from disk into main memory, so the dataset has to
    be limited in size.
    """
    def __init__(self, directory: str, n: int, read_length: int, floor = 1000):
        total_reads = sum(get_n_reads_in_compressed_dataset(directory))
        ids = get_floored_random_identifiers(directory, floor, n)
        self.n = n
        self.x, self.y = sample_compressed_dataset(
            directory,
            total_reads,
            read_length,
            ids,
            0
        )

    def __len__(self):
        return self.n

    def __getitem__(self, idx):
        x = self.x[idx]
        y = self.y[idx]
        return tensor(x), tensor(y).to(int)


class CompressedReadDataset(Dataset):
    """Dataset containing 4mer-encoded synthetic reads split in multiple files.

    The directory containing the reads is expected to contain the files:
    - `<x>_x.npy`, where `<x>` is an integer.
    - `<y>_y.npy`, where `<y>` is an integer.
    - `counts.json`

    Data are loaded from disk into main memory in chunks, so the full dataset
    can be arbitrarily large.
    """
    def __init__(
            self,
            directory: str,
            partition_size: int,
            n_max_repeats: int
            ):
        """
        Args:
            directory: Path of the directory that contains the x and y files.
            fragment: Number of reads in a partition of the dataset.
            n_max_repeats: Maximum number of times that a fragment is allowed
                to be repeated to balance a dataset.
        """
        self.directory = directory
        self.offset = 0
        self.partition_size = partition_size
        self.n = sum(get_n_reads_in_compressed_dataset(directory))
        self.identifiers_by_class = get_identifiers_by_class(directory)
        self.frequencies = {
            i: len(c) / self.n for i, c in self.identifiers_by_class.items()
        }
        probe = np.load(directory + "0_x.npy")
        self.read_length = probe.shape[1] * 4
        self.n_max_repeats = n_max_repeats
        self.set_identifiers(self.frequencies, n_max_repeats)
        self.load_partition()

    def set_identifiers(self, frequencies, n_max_repeats) -> None:
        identifier_list = []
        threshold = max(frequencies.values()) * 0.25
        reference = sum(frequencies.values()) / len(frequencies)
        max_delta = reference - min(frequencies.values())
        for frequency, i in zip(frequencies.values(), self.identifiers_by_class):
            if frequency > threshold:
                identifier_list += self.identifiers_by_class[i]
            else:
                delta = reference - frequency
                factor = delta / max_delta
                n_repeats = int(factor * n_max_repeats)
                identifier_list += self.identifiers_by_class[i] * n_repeats
        shuffle(identifier_list)
        self.identifiers = {
            i: r for i, r in zip(range(self.n), identifier_list)
        }

    def load_partition(self):
        if self.offset + self.partition_size >= self.n:
            partition_size = self.n - self.offset
        else:
            partition_size = self.partition_size
        self.x, self.y = sample_compressed_dataset(
            self.directory,
            partition_size,
            self.read_length,
            self.identifiers,
            self.offset
        )
        self.offset += self.partition_size
        if self.offset >= self.n:
            self.offset = 0

    def __len__(self):
        return self.n

    def reset(self):
        self.offset = 0
        self.load_partition()

    def __getitem__(self, idx):
        local_index = idx - self.offset
        if local_index >= self.partition_size:
            self.load_partition()
            local_index = 0
        return tensor(self.x[local_index]), tensor(self.y[local_index]).to(int)


def sample_compressed_dataset_no_offset(
        directory: str,
        read_length: int,
        identifiers: dict,
        ) -> tuple[np.ndarray]:
    """Obtain reads from a compressed dataset.

    Args:
        directory: Directory that contains the compressed data.
        n: Number of reads to sample.
        read_length: Length of the reads.
        identifiers: Map generated by the function `get_random_identifiers`.
        offset: Index at which to start reading identifiers.
    """
    assert directory.endswith("/")
    n = len(identifiers)
    x = np.zeros((n, int(read_length / 4)), dtype=np.uint8)
    y = np.zeros(n, dtype=np.uint16)
    with open(directory + "counts.json", "r") as file:
        counts = json.load(file)
    global_offset = 0
    for identifier, count in counts.items():
        partial_x = np.load(directory + str(identifier) + "_x.npy")
        partial_x = partial_x[:count]
        partial_y = np.load(directory + str(identifier) + "_y.npy")
        partial_y = partial_y[:count]
        for i, global_src_index in identifiers.items():
            local_src_index = global_src_index - global_offset
            if 0 <= local_src_index < count:
                x[i] = partial_x[local_src_index]
                y[i] = partial_y[local_src_index]
        global_offset += count
    return x, y


class AdaptiveCompressedReadDataset(Dataset):
    """Dataset containing 4mer-encoded synthetic reads split in multiple files
    that can be adapted at runtime.

    The directory containing the reads is expected to contain the files:
    - `<x>_x.npy`, where `<x>` is an integer.
    - `<y>_y.npy`, where `<y>` is an integer.
    - `counts.json`

    Data are loaded from disk into main memory in chunks, so the full dataset
    can be arbitrarily large.
    """
    def __init__(
            self,
            directory: str,
            partition_size: int,
            distributions: tuple[float] = None,
            ):
        """
        Args:
            directory: Path of the directory that contains the x and y files.
            fragment: Number of reads in a partition of the dataset.
        """
        self.directory = directory
        self.offset = 0
        self.partition_size = partition_size
        self.identifiers_by_class = get_identifiers_by_class(directory)
        self.offset_by_class = [0 for _ in range(len(self.identifiers_by_class))]
        self.n = sum([len(c) for _, c in self.identifiers_by_class.items()])
        probe = np.load(directory + "0_x.npy")
        self.read_length = probe.shape[1] * 4
        if not distributions:
            m = len(self.identifiers_by_class)
            distributions = [1 / m for _ in range(m)]
        self.set_distribution(distributions)
        self.load_partition()

    def set_distribution(self, distributions) -> None:
        self.distributions = distributions

    def set_identifiers(self, frequencies) -> None:
        identifier_list = []
        for frequency, i in zip(frequencies, self.identifiers_by_class):
            weight = len(self.identifiers_by_class) / self.n
            amount = int(frequency * self.partition_size) + 0.5 * weight
            identifier_list += choices(self.identifiers_by_class[i], k=n)
        shuffle(identifier_list)
        if len(identifier_list) < self.partition_size:
            delta = self.partition_size - len(identifier_list)
            identifier_list += choices(identifier_list, k=delta)
        self.identifiers = {
            i: r for i, r in zip(range(self.partition_size), identifier_list)
        }

    def load_partition(self):
        self.set_identifiers(self.distributions)
        self.x, self.y = sample_compressed_dataset_no_offset(
            self.directory,
            self.read_length,
            self.identifiers,
        )

    def __len__(self):
        return self.n

    def reset(self):
        self.offset = 0
        self.load_partition()

    def __getitem__(self, idx):
        local_index = idx - self.offset
        if local_index >= self.partition_size:
            self.load_partition()
            self.offset += self.partition_size
            if self.offset >= self.n:
                self.offset = 0
            local_index = 0
        return tensor(self.x[local_index]), tensor(self.y[local_index]).to(int)
